{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:11:12.707979Z",
     "start_time": "2021-01-10T15:11:12.701089Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "#findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:11:15.142161Z",
     "start_time": "2021-01-10T15:11:15.073416Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:11:21.712321Z",
     "start_time": "2021-01-10T15:11:17.994869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure spark session\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[2]')\\\n",
    "    .appName('quake_etl')\\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:2.4.1')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T09:00:19.883555Z",
     "start_time": "2021-01-10T09:00:18.905608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Using cached findspark-1.4.2-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:11:31.033682Z",
     "start_time": "2021-01-10T15:11:28.911505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='01/02/1965', Time='13:44:18', Latitude='19.246', Longitude='145.616', Type='Earthquake', Depth='131.6', Depth Error=None, Depth Seismic Stations=None, Magnitude='6', Magnitude Type='MW', Magnitude Error=None, Magnitude Seismic Stations=None, Azimuthal Gap=None, Horizontal Distance=None, Horizontal Error=None, Root Mean Square=None, ID='ISCGEM860706', Source='ISCGEM', Location Source='ISCGEM', Magnitude Source='ISCGEM', Status='Automatic')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset \n",
    "df_load = spark.read.csv(r\"database.csv\", header=True)\n",
    "# Preview df_load\n",
    "df_load.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:11:34.084996Z",
     "start_time": "2021-01-10T15:11:33.912008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|        6|            MW|ISCGEM860706|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake|   80|      5.8|            MW|ISCGEM860737|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake|   20|      6.2|            MW|ISCGEM860762|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake|   15|      5.8|            MW|ISCGEM860856|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake|   15|      5.8|            MW|ISCGEM860890|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop fields we don't need from df_load\n",
    "lst_dropped_columns = ['Depth Error', 'Time', 'Depth Seismic Stations','Magnitude Error','Magnitude Seismic Stations','Azimuthal Gap', 'Horizontal Distance','Horizontal Error',\n",
    "    'Root Mean Square','Source','Location Source','Magnitude Source','Status']\n",
    "\n",
    "df_load = df_load.drop(*lst_dropped_columns)\n",
    "# Preview df_load\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:11:38.833080Z",
     "start_time": "2021-01-10T15:11:38.611863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|        6|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake|   80|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake|   20|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake|   15|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake|   15|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a year field and add it to the dataframe\n",
    "df_load = df_load.withColumn('Year', year(to_timestamp('Date', 'dd/MM/yyyy')))\n",
    "# Preview df_load\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:11:43.290241Z",
     "start_time": "2021-01-10T15:11:42.094352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|Year|Counts|\n",
      "+----+------+\n",
      "|1990|   196|\n",
      "|1975|   150|\n",
      "|1977|   148|\n",
      "|2003|   187|\n",
      "|2007|   211|\n",
      "+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the quakes frequency dataframe using the year field and counts for each year\n",
    "df_quake_freq = df_load.groupBy('Year').count().withColumnRenamed('count', 'Counts')\n",
    "# Preview df_quake_freq\n",
    "df_quake_freq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:11:47.191183Z",
     "start_time": "2021-01-10T15:11:47.182353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Depth: string (nullable = true)\n",
      " |-- Magnitude: string (nullable = true)\n",
      " |-- Magnitude Type: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview df_load schema\n",
    "df_load.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:11:50.555976Z",
     "start_time": "2021-01-10T15:11:50.405220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|      6.0|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake| 80.0|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake| 20.0|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake| 15.0|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake| 15.0|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cast some fields from string into numeric types\n",
    "df_load = df_load.withColumn('Latitude', df_load['Latitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Longitude', df_load['Longitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Depth', df_load['Depth'].cast(DoubleType()))\\\n",
    "    .withColumn('Magnitude', df_load['Magnitude'].cast(DoubleType()))\n",
    "\n",
    "# Preview df_load\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:11:53.900797Z",
     "start_time": "2021-01-10T15:11:53.890514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Depth: double (nullable = true)\n",
      " |-- Magnitude: double (nullable = true)\n",
      " |-- Magnitude Type: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview df_load schema\n",
    "df_load.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:11:57.394379Z",
     "start_time": "2021-01-10T15:11:57.346029Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create avg magnitude and max magnitude fields and add to df_quake_freq\n",
    "df_max = df_load.groupBy('Year').max('Magnitude').withColumnRenamed('max(Magnitude)', 'Max_Magnitude')\n",
    "df_avg = df_load.groupBy('Year').avg('Magnitude').withColumnRenamed('avg(Magnitude)', 'Avg_Magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:12:02.535258Z",
     "start_time": "2021-01-10T15:12:00.459882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------------+-------------+\n",
      "|Year|Counts|    Avg_Magnitude|Max_Magnitude|\n",
      "+----+------+-----------------+-------------+\n",
      "|1990|   196|5.858163265306125|          7.6|\n",
      "|1975|   150| 5.84866666666667|          7.8|\n",
      "|1977|   148|5.757432432432437|          7.6|\n",
      "|2003|   187|5.850802139037435|          7.6|\n",
      "|2007|   211| 5.89099526066351|          8.4|\n",
      "+----+------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join df_max, and df_avg to df_quake_freq\n",
    "df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max, ['Year'])\n",
    "# Preview df_quake_freq\n",
    "df_quake_freq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:12:05.790826Z",
     "start_time": "2021-01-10T15:12:05.752953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Counts: bigint, Avg_Magnitude: double, Max_Magnitude: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove nulls\n",
    "df_load.dropna()\n",
    "df_quake_freq.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:12:11.707535Z",
     "start_time": "2021-01-10T15:12:11.608035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|      6.0|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake| 80.0|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake| 20.0|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake| 15.0|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake| 15.0|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview dataframes\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:12:17.424305Z",
     "start_time": "2021-01-10T15:12:16.006319Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------------+-------------+\n",
      "|Year|Counts|    Avg_Magnitude|Max_Magnitude|\n",
      "+----+------+-----------------+-------------+\n",
      "|1990|   196|5.858163265306125|          7.6|\n",
      "|1975|   150| 5.84866666666667|          7.8|\n",
      "|1977|   148|5.757432432432437|          7.6|\n",
      "|2003|   187|5.850802139037435|          7.6|\n",
      "|2007|   211| 5.89099526066351|          8.4|\n",
      "+----+------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_quake_freq.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> run the command \n",
    "\n",
    "`spark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/Quake.quakes?readPreference=primaryPreferred\" \n",
    "--conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/Quake.quakes\"\n",
    "--packages org.mongodb.spark:mongo-spark-connector_2.12:2.4.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:40:43.307299Z",
     "start_time": "2021-01-10T15:40:42.394510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the tables/collections in mongodb\n",
    "# Write df_load to mongodb\n",
    "df_load.write.format('mongo')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017/Quake.quakes').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:44:09.851796Z",
     "start_time": "2021-01-10T15:44:08.014494Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write df_quake_freq to mongodb\n",
    "df_quake_freq.write.format('mongo')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017/Quake.quake_freq').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Pyspark and MLlib\n",
    "\n",
    "\n",
    "Get data from: https://github.com/EBISYS/WaterWatch/blob/master/query.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:48:42.275719Z",
     "start_time": "2021-01-10T15:48:42.090361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(time='2017-01-02T00:13:06.300Z', latitude='-36.0365', longitude='51.9288', depth='10', mag='5.7', magType='mwb', nst=None, gap='26', dmin='14.685', rms='1.37', net='us', id='us10007p5d', updated='2017-03-27T23:53:17.040Z', place='Southwest Indian Ridge', type='earthquake', horizontalError='10.3', depthError='1.7', magError='0.068', magNst='21', status='reviewed', locationSource='us', magSource='us')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test data file into a dataframe\n",
    "df_test = spark.read.csv(r\"query.csv\", header=True)\n",
    "# Preview df_test\n",
    "df_test.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:51:31.978876Z",
     "start_time": "2021-01-10T15:51:31.655720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------+--------+---------+---------+--------------+----------+----+--------------------+\n",
      "|      Date|Depth|          ID|Latitude|Longitude|Magnitude|Magnitude Type|      Type|Year|                 _id|\n",
      "+----------+-----+------------+--------+---------+---------+--------------+----------+----+--------------------+\n",
      "|01/02/1965|131.6|ISCGEM860706|  19.246|  145.616|      6.0|            MW|Earthquake|1965|[5ffb1ffad5f36472...|\n",
      "|01/04/1965| 80.0|ISCGEM860737|   1.863|  127.352|      5.8|            MW|Earthquake|1965|[5ffb1ffad5f36472...|\n",
      "|01/05/1965| 20.0|ISCGEM860762| -20.579| -173.972|      6.2|            MW|Earthquake|1965|[5ffb1ffad5f36472...|\n",
      "|01/08/1965| 15.0|ISCGEM860856| -59.076|  -23.557|      5.8|            MW|Earthquake|1965|[5ffb1ffad5f36472...|\n",
      "|01/09/1965| 15.0|ISCGEM860890|  11.938|  126.427|      5.8|            MW|Earthquake|1965|[5ffb1ffad5f36472...|\n",
      "+----------+-----+------------+--------+---------+---------+--------------+----------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the training data from mongo into a dataframe\n",
    "df_train = spark.read.format('mongo')\\\n",
    "    .option('spark.mongodb.input.uri', 'mongodb://127.0.0.1:27017/Quake.quakes').load()\n",
    "\n",
    "# Preview df_train\n",
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T15:52:39.943923Z",
     "start_time": "2021-01-10T15:52:39.840188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+---+------+\n",
      "|                time|latitude|longitude|mag| depth|\n",
      "+--------------------+--------+---------+---+------+\n",
      "|2017-01-02T00:13:...|-36.0365|  51.9288|5.7|    10|\n",
      "|2017-01-02T13:13:...|  -4.895| -76.3675|5.9|   106|\n",
      "|2017-01-02T13:14:...|-23.2513| 179.2383|6.3|551.62|\n",
      "|2017-01-03T09:09:...| 24.0151|  92.0177|5.7|    32|\n",
      "|2017-01-03T21:19:...|-43.3527| -74.5017|5.5| 10.26|\n",
      "+--------------------+--------+---------+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select fields we will use and discard fields we don't need\n",
    "df_test_clean = df_test['time', 'latitude', 'longitude', 'mag', 'depth']\n",
    "# Preview df_test_clean\n",
    "df_test_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:03:43.189643Z",
     "start_time": "2021-01-10T16:03:43.107980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+---------+------+\n",
      "|                Date|Latitude|Longitude|Magnitude| Depth|\n",
      "+--------------------+--------+---------+---------+------+\n",
      "|2017-01-02T00:13:...|-36.0365|  51.9288|      5.7|    10|\n",
      "|2017-01-02T13:13:...|  -4.895| -76.3675|      5.9|   106|\n",
      "|2017-01-02T13:14:...|-23.2513| 179.2383|      6.3|551.62|\n",
      "|2017-01-03T09:09:...| 24.0151|  92.0177|      5.7|    32|\n",
      "|2017-01-03T21:19:...|-43.3527| -74.5017|      5.5| 10.26|\n",
      "+--------------------+--------+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename fields to be similar to test data \n",
    "df_test_clean = df_test_clean.withColumnRenamed('time', 'Date')\\\n",
    "    .withColumnRenamed('latitude', 'Latitude')\\\n",
    "    .withColumnRenamed('longitude', 'Longitude')\\\n",
    "    .withColumnRenamed('mag', 'Magnitude')\\\n",
    "    .withColumnRenamed('depth', 'Depth')\n",
    "\n",
    "# Preview df_test_clean\n",
    "df_test_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:04:10.828894Z",
     "start_time": "2021-01-10T16:04:10.822249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Magnitude: string (nullable = true)\n",
      " |-- Depth: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview Schema\n",
    "df_test_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:05:36.339086Z",
     "start_time": "2021-01-10T16:05:36.300870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cast some string fields into numeric fields\n",
    "df_test_clean = df_test_clean.withColumn('Latitude', df_test_clean['Latitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Longitude', df_test_clean['Longitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Depth', df_test_clean['Depth'].cast(DoubleType()))\\\n",
    "    .withColumn('Magnitude', df_test_clean['Magnitude'].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:05:41.696864Z",
     "start_time": "2021-01-10T16:05:41.687900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Magnitude: double (nullable = true)\n",
      " |-- Depth: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:05:50.982967Z",
     "start_time": "2021-01-10T16:05:50.949553Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create training and testing dataframes\n",
    "df_testing = df_test_clean['Latitude', 'Longitude', 'Magnitude', 'Depth']\n",
    "df_training = df_train['Latitude', 'Longitude', 'Magnitude', 'Depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:05:55.405658Z",
     "start_time": "2021-01-10T16:05:55.297840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+-----+\n",
      "|Latitude|Longitude|Magnitude|Depth|\n",
      "+--------+---------+---------+-----+\n",
      "|  19.246|  145.616|      6.0|131.6|\n",
      "|   1.863|  127.352|      5.8| 80.0|\n",
      "| -20.579| -173.972|      6.2| 20.0|\n",
      "| -59.076|  -23.557|      5.8| 15.0|\n",
      "|  11.938|  126.427|      5.8| 15.0|\n",
      "+--------+---------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview df_training\n",
    "df_training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:05:59.048713Z",
     "start_time": "2021-01-10T16:05:58.948904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------+\n",
      "|Latitude|Longitude|Magnitude| Depth|\n",
      "+--------+---------+---------+------+\n",
      "|-36.0365|  51.9288|      5.7|  10.0|\n",
      "|  -4.895| -76.3675|      5.9| 106.0|\n",
      "|-23.2513| 179.2383|      6.3|551.62|\n",
      "| 24.0151|  92.0177|      5.7|  32.0|\n",
      "|-43.3527| -74.5017|      5.5| 10.26|\n",
      "+--------+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview df_testing\n",
    "df_testing.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:06:09.031813Z",
     "start_time": "2021-01-10T16:06:09.002097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop records with null values from our dataframes\n",
    "df_testing = df_testing.dropna()\n",
    "df_training = df_training.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:08:20.351459Z",
     "start_time": "2021-01-10T16:08:20.345366Z"
    }
   },
   "outputs": [],
   "source": [
    "# building predictive model \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:11:11.851309Z",
     "start_time": "2021-01-10T16:11:09.822646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select features to parse into our model and then create the feature vector\n",
    "assembler = VectorAssembler(inputCols=['Latitude', 'Longitude', 'Depth'], outputCol='features')\n",
    "\n",
    "# Create the Model\n",
    "model_reg = RandomForestRegressor(featuresCol='features', labelCol='Magnitude')\n",
    "\n",
    "# Chain the assembler with the model in a pipeline\n",
    "pipeline = Pipeline(stages=[assembler, model_reg])\n",
    "\n",
    "# Train the Model\n",
    "model = pipeline.fit(df_training)\n",
    "\n",
    "# Make the prediction\n",
    "pred_results = model.transform(df_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:11:18.428510Z",
     "start_time": "2021-01-10T16:11:18.314422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------+--------------------+-----------------+\n",
      "|Latitude|Longitude|Magnitude| Depth|            features|       prediction|\n",
      "+--------+---------+---------+------+--------------------+-----------------+\n",
      "|-36.0365|  51.9288|      5.7|  10.0|[-36.0365,51.9288...|5.838643911900222|\n",
      "|  -4.895| -76.3675|      5.9| 106.0|[-4.895,-76.3675,...|5.887379372680598|\n",
      "|-23.2513| 179.2383|      6.3|551.62|[-23.2513,179.238...|5.892521857359297|\n",
      "| 24.0151|  92.0177|      5.7|  32.0|[24.0151,92.0177,...|5.912412989642489|\n",
      "|-43.3527| -74.5017|      5.5| 10.26|[-43.3527,-74.501...|5.960231968968556|\n",
      "+--------+---------+---------+------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview pred_results dataframe\n",
    "pred_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:11:26.077942Z",
     "start_time": "2021-01-10T16:11:25.898365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.402681\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "# rmse should be less than 0.5 for the model to be useful\n",
    "evaluator = RegressionEvaluator(labelCol='Magnitude', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(pred_results)\n",
    "print('Root Mean Squared Error (RMSE) on test data = %g' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create the prediction dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:17:44.938197Z",
     "start_time": "2021-01-10T16:17:44.828329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------+----+-------------------+\n",
      "|Latitude|Longitude|   Pred_Magnitude|Year|               RMSE|\n",
      "+--------+---------+-----------------+----+-------------------+\n",
      "|-36.0365|  51.9288|5.838643911900222|2017|0.40268138024273575|\n",
      "|  -4.895| -76.3675|5.887379372680598|2017|0.40268138024273575|\n",
      "|-23.2513| 179.2383|5.892521857359297|2017|0.40268138024273575|\n",
      "| 24.0151|  92.0177|5.912412989642489|2017|0.40268138024273575|\n",
      "|-43.3527| -74.5017|5.960231968968556|2017|0.40268138024273575|\n",
      "+--------+---------+-----------------+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the prediction dataset\n",
    "df_pred_results = pred_results['Latitude', 'Longitude', 'prediction']\n",
    "\n",
    "# Rename the prediction field\n",
    "df_pred_results = df_pred_results.withColumnRenamed('prediction', 'Pred_Magnitude')\n",
    "\n",
    "# Add more columns to our prediction dataset\n",
    "df_pred_results = df_pred_results.withColumn('Year', lit(2017))\\\n",
    "    .withColumn('RMSE', lit(rmse))\n",
    "\n",
    "# Preview df_pred_results\n",
    "df_pred_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:20:05.572479Z",
     "start_time": "2021-01-10T16:20:05.377209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the prediction dataset into mongodb\n",
    "# Write df_pred_results\n",
    "df_pred_results.write.format('mongo')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017/Quake.pred_results').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visulization\n",
    "\n",
    "- Loading data source from MongoDB\n",
    "- Create Map plot \n",
    "- Create a Bat chart \n",
    "- create a grid plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:26:29.035213Z",
     "start_time": "2021-01-10T16:26:28.956900Z"
    }
   },
   "outputs": [],
   "source": [
    "# To create a dashboard we use Python \n",
    "\n",
    "import pandas as pd\n",
    "from bokeh.io import output_notebook, output_file\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models.tools import HoverTool\n",
    "import math\n",
    "from math import pi\n",
    "from bokeh.palettes import Category20c\n",
    "from bokeh.transform import cumsum\n",
    "from bokeh.tile_providers import CARTODBPOSITRON\n",
    "from bokeh.themes import built_in_themes\n",
    "from bokeh.io import curdoc\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:26:24.902473Z",
     "start_time": "2021-01-10T16:26:22.956307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-3.11.2-cp37-cp37m-manylinux2014_x86_64.whl (512 kB)\n",
      "\u001b[K     |████████████████████████████████| 512 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pymongo\n",
      "Successfully installed pymongo-3.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:39:53.000526Z",
     "start_time": "2021-01-10T16:39:52.995017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a custom read function to read data from mongodb into a dataframe\n",
    "def read_mongo(host='127.0.0.1', port=27017, username=None, password=None, db='Quake', collection='pred_results'):\n",
    "    \n",
    "    mongo_uri = 'mongodb://{}:{}/{}.{}'.format(host, port, db, collection)\n",
    "    \n",
    "    # Connect to mongodb\n",
    "    conn = MongoClient(mongo_uri)\n",
    "    db = conn[db]\n",
    "    \n",
    "    # Select all records from the collection\n",
    "    cursor = db[collection].find()\n",
    "    \n",
    "    # Create the dataframe\n",
    "    df = pd.DataFrame(list(cursor))\n",
    "    \n",
    "    # Delete the _id field\n",
    "    del df['_id']\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:40:00.812012Z",
     "start_time": "2021-01-10T16:40:00.646174Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the datasets from mongodb\n",
    "df_quakes = read_mongo(collection='quakes')\n",
    "df_quake_freq = read_mongo(collection='quake_freq')\n",
    "df_quake_pred = read_mongo(collection='pred_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:40:07.532358Z",
     "start_time": "2021-01-10T16:40:07.483377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Type</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Magnitude Type</th>\n",
       "      <th>ID</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22943</th>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>-50.5575</td>\n",
       "      <td>139.4489</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.3</td>\n",
       "      <td>MWW</td>\n",
       "      <td>US10004ANT</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22944</th>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>-28.6278</td>\n",
       "      <td>-177.2810</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>34.00</td>\n",
       "      <td>5.8</td>\n",
       "      <td>MWW</td>\n",
       "      <td>US10004AQY</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22945</th>\n",
       "      <td>01/02/2016</td>\n",
       "      <td>44.8069</td>\n",
       "      <td>129.9406</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>585.47</td>\n",
       "      <td>5.8</td>\n",
       "      <td>MWW</td>\n",
       "      <td>US10004ATB</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22946</th>\n",
       "      <td>01/03/2016</td>\n",
       "      <td>24.8036</td>\n",
       "      <td>93.6505</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>55.00</td>\n",
       "      <td>6.7</td>\n",
       "      <td>MWW</td>\n",
       "      <td>US10004B2N</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22947</th>\n",
       "      <td>01/05/2016</td>\n",
       "      <td>30.6132</td>\n",
       "      <td>132.7337</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>4.71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>MWW</td>\n",
       "      <td>US10004BEN</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  Latitude  Longitude        Type   Depth  Magnitude  \\\n",
       "22943  01/01/2016  -50.5575   139.4489  Earthquake   10.00        6.3   \n",
       "22944  01/01/2016  -28.6278  -177.2810  Earthquake   34.00        5.8   \n",
       "22945  01/02/2016   44.8069   129.9406  Earthquake  585.47        5.8   \n",
       "22946  01/03/2016   24.8036    93.6505  Earthquake   55.00        6.7   \n",
       "22947  01/05/2016   30.6132   132.7337  Earthquake    4.71        5.8   \n",
       "\n",
       "      Magnitude Type          ID    Year  \n",
       "22943            MWW  US10004ANT  2016.0  \n",
       "22944            MWW  US10004AQY  2016.0  \n",
       "22945            MWW  US10004ATB  2016.0  \n",
       "22946            MWW  US10004B2N  2016.0  \n",
       "22947            MWW  US10004BEN  2016.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quakes_2016 = df_quakes[df_quakes['Year'] == 2016]\n",
    "# Preview df_quakes_2016\n",
    "df_quakes_2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:42:48.471073Z",
     "start_time": "2021-01-10T16:42:48.453358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show plots embedded in jupyter notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T16:43:09.181257Z",
     "start_time": "2021-01-10T16:43:09.175978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create custom style function to style our plots\n",
    "def style(p):\n",
    "    # Title\n",
    "    p.title.align='center'\n",
    "    p.title.text_font_size='20pt'\n",
    "    p.title.text_font='serif'\n",
    "    \n",
    "    # Axis titles\n",
    "    p.xaxis.axis_label_text_font_size='14pt'\n",
    "    p.xaxis.axis_label_text_font_style='bold'\n",
    "    p.yaxis.axis_label_text_font_size='14pt'\n",
    "    p.yaxis.axis_label_text_font_style='bold'\n",
    "    \n",
    "    # Tick labels\n",
    "    p.xaxis.major_label_text_font_size='12pt'\n",
    "    p.yaxis.major_label_text_font_size='12pt'\n",
    "    \n",
    "    # Plot the legend in the top left corner\n",
    "    p.legend.location='top_left'\n",
    "    \n",
    "    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:18:04.454170Z",
     "start_time": "2021-01-10T17:18:04.432610Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Geo Map plot\n",
    "def plotMap():\n",
    "    lat = df_quakes_2016['Latitude'].values.tolist()\n",
    "    lon = df_quakes_2016['Longitude'].values.tolist()\n",
    "    \n",
    "    pred_lat = df_quake_pred['Latitude'].values.tolist()\n",
    "    pred_lon = df_quake_pred['Longitude'].values.tolist()\n",
    "    \n",
    "    lst_lat = []\n",
    "    lst_lon = []\n",
    "    lst_pred_lat = []\n",
    "    lst_pred_lon = []\n",
    "    \n",
    "    i=0\n",
    "    j=0\n",
    "    \n",
    "    # Convert Lat and Long values into merc_projection format\n",
    "    for i in range(len(lon)):\n",
    "        r_major = 6378137.000\n",
    "        x = r_major * math.radians(lon[i])\n",
    "        scale = x/lon[i]\n",
    "        y = 180.0/math.pi * math.log(math.tan(math.pi/4.0 +\n",
    "            lat[i] * (math.pi/180.0)/2.0)) * scale\n",
    "        \n",
    "        lst_lon.append(x)\n",
    "        lst_lat.append(y)\n",
    "        i += 1\n",
    "        \n",
    "    # Convert predicted lat and long values into merc_projection format\n",
    "    for j in range(len(pred_lon)):\n",
    "        r_major = 6378137.000\n",
    "        x = r_major * math.radians(pred_lon[j])\n",
    "        scale = x/pred_lon[j]\n",
    "        y = 180.0/math.pi * math.log(math.tan(math.pi/4.0 +\n",
    "            pred_lat[j] * (math.pi/180.0)/2.0)) * scale\n",
    "        \n",
    "        lst_pred_lon.append(x)\n",
    "        lst_pred_lat.append(y)\n",
    "        j += 1\n",
    "    \n",
    "    \n",
    "    df_quakes_2016['coords_x'] = lst_lat\n",
    "    df_quakes_2016['coords_y'] = lst_lon\n",
    "    df_quake_pred['coords_x'] = lst_pred_lat\n",
    "    df_quake_pred['coords_y'] = lst_pred_lon\n",
    "    \n",
    "    # Scale the circles\n",
    "    df_quakes_2016['Mag_Size'] = df_quakes_2016['Magnitude'] * 4\n",
    "    df_quake_pred['Mag_Size'] = df_quake_pred['Pred_Magnitude'] * 4\n",
    "    \n",
    "    # create datasources for our ColumnDataSource object\n",
    "    lats = df_quakes_2016['coords_x'].tolist()\n",
    "    longs = df_quakes_2016['coords_y'].tolist()\n",
    "    mags = df_quakes_2016['Magnitude'].tolist()\n",
    "    years = df_quakes_2016['Year'].tolist()\n",
    "    mag_size = df_quakes_2016['Mag_Size'].tolist()\n",
    "    \n",
    "    pred_lats = df_quake_pred['coords_x'].tolist()\n",
    "    pred_longs = df_quake_pred['coords_y'].tolist()\n",
    "    pred_mags = df_quake_pred['Pred_Magnitude'].tolist()\n",
    "    pred_year = df_quake_pred['Year'].tolist()\n",
    "    pred_mag_size = df_quake_pred['Mag_Size'].tolist()\n",
    "    \n",
    "    # Create column datasource\n",
    "    cds = ColumnDataSource(\n",
    "        data=dict(\n",
    "            lat=lats,\n",
    "            lon=longs,\n",
    "            mag=mags,\n",
    "            year=years,\n",
    "            mag_s=mag_size\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    pred_cds = ColumnDataSource(\n",
    "        data=dict(\n",
    "            pred_lat=pred_lats,\n",
    "            pred_long=pred_longs,\n",
    "            pred_mag=pred_mags,\n",
    "            year=pred_year,\n",
    "            pred_mag_s=pred_mag_size\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Tooltips\n",
    "    TOOLTIPS = [\n",
    "        (\"Year\", \" @year\"),\n",
    "        (\"Magnitude\", \" @mag\"),\n",
    "        (\"Predicted Magnitude\", \" @pred_mag\")\n",
    "    ]\n",
    "    \n",
    "    # Create figure\n",
    "    p = figure(title='Earthquake Map',\n",
    "              plot_width=2300, plot_height=450,\n",
    "              x_range=(-2000000, 6000000),\n",
    "              y_range=(-1000000, 7000000),\n",
    "              tooltips=TOOLTIPS)\n",
    "    \n",
    "    p.circle(x='lon', y='lat', size='mag_s', fill_color='#cc0000', fill_alpha=0.7,\n",
    "            source=cds, legend='Quakes 2016')\n",
    "    \n",
    "    # Add circles for our predicted earthquakes\n",
    "    p.circle(x='pred_long', y='pred_lat', size='pred_mag_s', fill_color='#ccff33', fill_alpha=7.0,\n",
    "            source=pred_cds, legend='Predicted Quakes 2017')\n",
    "    \n",
    "    p.add_tile(CARTODBPOSITRON)\n",
    "    \n",
    "    # Style the map plot\n",
    "    # Title\n",
    "    p.title.align='center'\n",
    "    p.title.text_font_size='20pt'\n",
    "    p.title.text_font='serif'\n",
    "    \n",
    "    # Legend\n",
    "    p.legend.location='bottom_right'\n",
    "    p.legend.background_fill_color='black'\n",
    "    p.legend.background_fill_alpha=0.8\n",
    "    p.legend.click_policy='hide'\n",
    "    p.legend.label_text_color='white'\n",
    "    p.xaxis.visible=False\n",
    "    p.yaxis.visible=False\n",
    "    p.axis.axis_label=None\n",
    "    p.axis.visible=False\n",
    "    p.grid.grid_line_color=None\n",
    "    \n",
    "    \n",
    "    #show(p)\n",
    "    \n",
    "    return p\n",
    "    \n",
    "#plotMap()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:18:14.148281Z",
     "start_time": "2021-01-10T17:18:14.139961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Bar Chart\n",
    "def plotBar():\n",
    "    # Load the datasource \n",
    "    cds = ColumnDataSource(data=dict(\n",
    "        yrs = df_quake_freq['Year'].values.tolist(),\n",
    "        numQuakes = df_quake_freq['Counts'].values.tolist()\n",
    "    ))\n",
    "    \n",
    "    # Tooltip\n",
    "    TOOLTIPS = [\n",
    "        ('Year', ' @yrs'),\n",
    "        ('Number of earthquakes', ' @numQuakes')\n",
    "    ]\n",
    "    \n",
    "    # Create a figure\n",
    "    barChart = figure(title='Frequency of Earthquakes by Year',\n",
    "                     plot_height=400,\n",
    "                     plot_width=1150,\n",
    "                     x_axis_label='Years',\n",
    "                     y_axis_label='Number of Occurances',\n",
    "                     x_minor_ticks=2,\n",
    "                     y_range=(0, df_quake_freq['Counts'].max() + 100),\n",
    "                     toolbar_location=None,\n",
    "                     tooltips=TOOLTIPS)\n",
    "    \n",
    "    # Create a vertical bar \n",
    "    barChart.vbar(x='yrs', bottom=0, top='numQuakes',\n",
    "                 color='#cc0000', width=0.75,\n",
    "                 legend='Year', source=cds)\n",
    "    \n",
    "    # Style the bar chart\n",
    "    barChart = style(barChart)\n",
    "    \n",
    "    #show(barChart)\n",
    "    \n",
    "    return barChart\n",
    "    \n",
    "    \n",
    "#plotBar()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:05:48.958591Z",
     "start_time": "2021-01-10T17:05:48.942962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Counts</th>\n",
       "      <th>Avg_Magnitude</th>\n",
       "      <th>Max_Magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1975</td>\n",
       "      <td>150</td>\n",
       "      <td>5.848667</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990</td>\n",
       "      <td>196</td>\n",
       "      <td>5.858163</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1977</td>\n",
       "      <td>148</td>\n",
       "      <td>5.757432</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>187</td>\n",
       "      <td>5.850802</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1974</td>\n",
       "      <td>147</td>\n",
       "      <td>5.890476</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Counts  Avg_Magnitude  Max_Magnitude\n",
       "0  1975     150       5.848667            7.8\n",
       "1  1990     196       5.858163            7.6\n",
       "2  1977     148       5.757432            7.6\n",
       "3  2003     187       5.850802            7.6\n",
       "4  1974     147       5.890476            7.6"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quake_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:18:25.769919Z",
     "start_time": "2021-01-10T17:18:25.759345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a magnitude plot\n",
    "def plotMagnitude():\n",
    "    # Load the datasource\n",
    "    cds = ColumnDataSource(data=dict(\n",
    "        yrs = df_quake_freq['Year'].values.tolist(),\n",
    "        avg_mag = df_quake_freq['Avg_Magnitude'].round(1).values.tolist(),\n",
    "        max_mag = df_quake_freq['Max_Magnitude'].values.tolist()\n",
    "    ))\n",
    "    \n",
    "    # Tooltip\n",
    "    TOOLTIPS = [\n",
    "        ('Year', ' @yrs'),\n",
    "        ('Average Magnitude', ' @avg_mag'),\n",
    "        ('Maximum Magnitude', ' @max_mag')\n",
    "    ]\n",
    "    \n",
    "    # Create the figure\n",
    "    mp = figure(title='Maximum and Average Magnitude by Year',\n",
    "               plot_width=1150, plot_height=400,\n",
    "               x_axis_label='Years',\n",
    "               y_axis_label='Magnitude',\n",
    "               x_minor_ticks=2,\n",
    "               y_range=(5, df_quake_freq['Max_Magnitude'].max() + 1),\n",
    "               toolbar_location=None,\n",
    "               tooltips=TOOLTIPS)\n",
    "    \n",
    "    # Max Magnitude\n",
    "    mp.line(x='yrs', y='max_mag', color='#cc0000', line_width=2, legend='Max Magnitude', source=cds)\n",
    "    mp.circle(x='yrs', y='max_mag', color='#cc0000', size=8, fill_color='#cc0000', source=cds)\n",
    "    \n",
    "    # Average Magnitude \n",
    "    mp.line(x='yrs', y='avg_mag', color='yellow', line_width=2, legend='Avg Magnitude', source=cds)\n",
    "    mp.circle(x='yrs', y='avg_mag', color='yellow', size=8, fill_color='yellow', source=cds)\n",
    "    \n",
    "    mp = style(mp)\n",
    "    \n",
    "    #show(mp)\n",
    "    \n",
    "    return mp\n",
    "\n",
    "#plotMagnitude()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
