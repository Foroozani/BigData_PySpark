{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c089c6",
   "metadata": {},
   "source": [
    " # <span class=\"burk\">Pyspark Datafram Master</span> \n",
    "\n",
    "## Spark DataFrame Basics\n",
    "\n",
    "Spark DataFrames are the workhouse and main way of working with Spark and Python post Spark 2.0. DataFrames act as powerful versions of tables, with rows and columns, easily handling large datasets. The shift to DataFrames provides many advantages:\n",
    "* A much simpler syntax\n",
    "* Ability to use SQL directly in the dataframe\n",
    "* Operations are automatically distributed across RDDs\n",
    "    \n",
    "If you've used R or even the pandas library with Python you are probably already familiar with the concept of DataFrames. Spark DataFrame expand on a lot of these concepts, allowing you to transfer that knowledge easily by understanding the simple syntax of Spark DataFrames. Remember that the main advantage to using Spark DataFrames vs those other programs is that Spark can handle data across many RDDs, huge data sets that would never fit on a single computer. That comes at a slight cost of some \"peculiar\" syntax choices, but after this course you will feel very comfortable with all those topics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8783207b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s) on appid:  local-1634565969155\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.6.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>master-session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10a3ad160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's create our PySpark instance\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"master-session\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "appid = spark._jsc.sc().applicationId()\n",
    "print(\"You are working with\", cores, \"core(s) on appid: \",appid)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f5c80",
   "metadata": {},
   "source": [
    "## Reading in data in different format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed991bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"../Datasets/\"\n",
    "\n",
    "# CSV\n",
    "df = spark.read.csv(path+'students.csv',inferSchema=True,header=True)\n",
    "\n",
    "# Json\n",
    "people = spark.read.json(path+'people.json')\n",
    "\n",
    "# Parquet\n",
    "parquet = spark.read.parquet(path+'users1.parquet')\n",
    "\n",
    "# Partioned Parquet\n",
    "partitioned = spark.read.parquet(path+'users*')\n",
    "\n",
    "# Parts of a partitioned Parquet 2 files here\n",
    "users1_2 = spark.read.option(\"basePath\", path).parquet(path+'users1.parquet', path+'users2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e39ff",
   "metadata": {},
   "source": [
    "**Parquet Files**\n",
    "\n",
    "Now try reading in a parquet file. This is most common data type in the big data world.\n",
    "Why? because it is the most compact file storage method (even better than zipped files!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42b94daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|           email|gender|    ip_address|              cc|  country|birthdate|   salary|           title|comments|\n",
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "|2016-02-03 08:55:29|  1|    Amanda|   Jordan|ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia| 3/8/1971| 49756.53|Internal Auditor|   1E+02|\n",
      "|2016-02-03 18:04:03|  2|    Albert|  Freeman| afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|1/16/1968|150280.17|   Accountant IV|        |\n",
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet = spark.read.parquet(path+'users1.parquet')\n",
    "parquet.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bae290",
   "metadata": {},
   "source": [
    "**Partitioned Parquet Files**\n",
    "\n",
    "Actually most big datasets will be partitioned. Here is how you can collect all the pieces (parts) of the dataset in one simple command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf1df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|           email|gender|    ip_address|              cc|  country|birthdate|   salary|           title|comments|\n",
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "|2016-02-03 08:55:29|  1|    Amanda|   Jordan|ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia| 3/8/1971| 49756.53|Internal Auditor|   1E+02|\n",
      "|2016-02-03 18:04:03|  2|    Albert|  Freeman| afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|1/16/1968|150280.17|   Accountant IV|        |\n",
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partitioned = spark.read.parquet(path+'users*')\n",
    "partitioned.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76eaac",
   "metadata": {},
   "source": [
    "<div class=\"mark\">\n",
    "You can also opt to read in only a specific set of paritioned parquet files. Say for example that you only wanted users1 and users2 and not users3</div><i class=\"fa fa-lightbulb-o \"></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38812e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|     ip_address|                 cc|             country| birthdate|   salary|               title|            comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|2016-02-03 08:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|    1.197.201.2|   6759521864920116|           Indonesia|  3/8/1971| 49756.53|    Internal Auditor|               1E+02|\n",
      "|2016-02-03 18:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male| 218.111.175.34|                   |              Canada| 1/16/1968|150280.17|       Accountant IV|                    |\n",
      "|2016-02-03 02:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|   7.161.136.94|   6767119071901597|              Russia|  2/1/1960|144972.51| Structural Engineer|                    |\n",
      "|2016-02-03 01:36:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female|  140.35.109.83|   3576031598965625|               China|  4/8/1997| 90263.05|Senior Cost Accou...|                    |\n",
      "|2016-02-03 06:05:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      | 169.113.235.40|   5602256255204850|        South Africa|          |     null|                    |                    |\n",
      "|2016-02-03 08:22:34|  6|   Kathryn|    White|  kwhite5@google.com|Female| 195.131.81.179|   3583136326049310|           Indonesia| 2/25/1983| 69227.11|   Account Executive|                    |\n",
      "|2016-02-03 09:33:08|  7|    Samuel|   Holmes|sholmes6@foxnews.com|  Male| 232.234.81.197|   3582641366974690|            Portugal|12/18/1987| 14247.62|Senior Financial ...|                    |\n",
      "|2016-02-03 07:47:06|  8|     Harry|   Howell| hhowell7@eepurl.com|  Male|   91.235.51.73|                   |Bosnia and Herzeg...|  3/1/1962|186469.43|    Web Developer IV|                    |\n",
      "|2016-02-03 04:52:53|  9|      Jose|   Foster|   jfoster8@yelp.com|  Male|   132.31.53.61|                   |         South Korea| 3/27/1992|231067.84|Software Test Eng...|               1E+02|\n",
      "|2016-02-03 19:29:47| 10|     Emily|  Stewart|estewart9@opensou...|Female| 143.28.251.245|   3574254110301671|             Nigeria| 1/28/1997| 27234.28|     Health Coach IV|                    |\n",
      "|2016-02-03 01:10:42| 11|     Susan|  Perkins| sperkinsa@patch.com|Female|    180.85.0.62|   3573823609854134|              Russia|          |210001.95|                    |                    |\n",
      "|2016-02-03 19:04:34| 12|     Alice|    Berry|aberryb@wikipedia...|Female| 246.225.12.189|   4917830851454417|               China| 8/12/1968| 22944.53|    Quality Engineer|                    |\n",
      "|2016-02-03 19:48:17| 13|    Justin|    Berry|jberryc@usatoday.com|  Male|   157.7.146.43|6331109912871813274|              Zambia| 8/15/1975| 44165.46|Structural Analys...|                    |\n",
      "|2016-02-03 22:46:52| 14|     Kathy| Reynolds|kreynoldsd@redcro...|Female|  81.254.172.13|   5537178462965976|Bosnia and Herzeg...| 6/27/1970|286592.99|           Librarian|                    |\n",
      "|2016-02-03 09:53:23| 15|   Dorothy|   Hudson|dhudsone@blogger.com|Female|       8.59.7.0|   3542586858224170|               Japan|12/20/1989|157099.71|  Nurse Practicioner|<script>alert('hi...|\n",
      "|2016-02-03 01:44:01| 16|     Bruce|   Willis|bwillisf@bluehost...|  Male|239.182.219.189|   3573030625927601|              Brazil|          |239100.65|                    |                    |\n",
      "|2016-02-03 01:57:45| 17|     Emily|  Andrews|eandrewsg@cornell...|Female| 29.231.180.172|     30271790537626|              Russia| 4/13/1990|116800.65|        Food Chemist|                    |\n",
      "|2016-02-03 17:44:24| 18|   Stephen|  Wallace|swallaceh@netvibe...|  Male|  152.49.213.62|   5433943468526428|             Ukraine| 1/15/1978|248877.99|Account Represent...|                    |\n",
      "|2016-02-03 12:45:54| 19|  Clarence|   Lawson|clawsoni@vkontakt...|  Male| 107.175.15.152|   3544052814080964|              Russia|          |177122.99|                    |                    |\n",
      "|2016-02-03 11:30:36| 20|   Rebecca|     Bell| rbellj@bandcamp.com|Female|172.215.104.127|                   |               China|          |137251.19|                    |                    |\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that the .option(\"basePath\", path) option is used to override the automatic function\n",
    "# that will exclude the partitioned variable in resulting dataframe. \n",
    "# I prefer to have the partitioning info in my new dataframe personally. \n",
    "users1_2 = spark.read.option(\"basePath\", path).parquet(path+'users1.parquet', path+'users2.parquet')\n",
    "users1_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba7501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2534214c",
   "metadata": {},
   "source": [
    "### read data from s3 bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee166e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #### If you're in AWS storing data in s3 buckets your code will more like this...\n",
    "\n",
    "bucket = \"my_bucket\"\n",
    "key1 = \"partition_test/Table1/CREATED_YEAR=2015/*\"\n",
    "key2 = \"partition_test/Table1/CREATED_YEAR=2017/*\"\n",
    "key3 = \"partition_test/Table1/CREATED_YEAR=2018/*\"\n",
    "\n",
    "test_df = spark.read.parquet('s3://'+bucket+'/'+key1, \\\n",
    "                             's3://'+bucket+'/'+key2, \\\n",
    "                             's3://'+bucket+'/'+key3)\n",
    "\n",
    "test_df.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502da6e3",
   "metadata": {},
   "source": [
    "### Notice the type differences here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf04b30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b22b13fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert it to Panda and write it back to new variale \n",
    "\n",
    "pandaDF = df.toPandas()\n",
    "type(pandaDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbb26cb",
   "metadata": {},
   "source": [
    "## Validate Schema and content at a glance\n",
    "\n",
    "Always a good idea to do this to ensure that dataframe was read in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "338c2d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \n",
       "0                    none          72             72             74  \n",
       "1               completed          69             90             88  \n",
       "2                    none          90             95             93  \n",
       "3                    none          47             57             44  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a434359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- race/ethnicity: string (nullable = true)\n",
      " |-- parental level of education: string (nullable = true)\n",
      " |-- lunch: string (nullable = true)\n",
      " |-- test preparation course: string (nullable = true)\n",
      " |-- math score: integer (nullable = true)\n",
      " |-- reading score: integer (nullable = true)\n",
      " |-- writing score: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a526c058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'race/ethnicity',\n",
       " 'parental level of education',\n",
       " 'lunch',\n",
       " 'test preparation course',\n",
       " 'math score',\n",
       " 'reading score',\n",
       " 'writing score']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or just the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a770011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gender', 'string'),\n",
       " ('race/ethnicity', 'string'),\n",
       " ('parental level of education', 'string'),\n",
       " ('lunch', 'string'),\n",
       " ('test preparation course', 'string'),\n",
       " ('math score', 'int'),\n",
       " ('reading score', 'int'),\n",
       " ('writing score', 'int')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Or just the data types which you can call on as a list\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ca8781f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringType"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or just the type of one column\n",
    "df.schema['lunch'].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "902fa049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        math score|\n",
      "+-------+------------------+\n",
      "|  count|              1000|\n",
      "|   mean|            66.089|\n",
      "| stddev|15.163080096009454|\n",
      "|    min|                 0|\n",
      "|    max|               100|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neat \"describe\" function\n",
    "df.describe(['math score']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96b1ca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+-------------+\n",
      "|summary|math score|reading score|writing score|\n",
      "+-------+----------+-------------+-------------+\n",
      "|  count|      1000|         1000|         1000|\n",
      "|    min|         0|           17|           10|\n",
      "|    25%|        57|           59|           57|\n",
      "|    75%|        77|           79|           79|\n",
      "|    max|       100|          100|          100|\n",
      "+-------+----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary function\n",
    "df.select(\"math score\", \"reading score\", \"writing score\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e4034",
   "metadata": {},
   "source": [
    "### read data with pre-datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9aa5d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordNumber</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>ZipCodeType</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>LocationType</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Xaxis</th>\n",
       "      <th>Yaxis</th>\n",
       "      <th>Zaxis</th>\n",
       "      <th>WorldRegion</th>\n",
       "      <th>Country</th>\n",
       "      <th>LocationText</th>\n",
       "      <th>Location</th>\n",
       "      <th>Decommisioned</th>\n",
       "      <th>TaxReturnsFiled</th>\n",
       "      <th>EstimatedPopulation</th>\n",
       "      <th>TotalWages</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>704</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>PARC PARQUE</td>\n",
       "      <td>PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>17.96</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Parc Parque, PR</td>\n",
       "      <td>NA-US-PR-PARC PARQUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>704</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>PASEO COSTA DEL SUR</td>\n",
       "      <td>PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>17.96</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Paseo Costa Del Sur, PR</td>\n",
       "      <td>NA-US-PR-PASEO COSTA DEL SUR</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>709</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>BDA SAN LUIS</td>\n",
       "      <td>PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>18.14</td>\n",
       "      <td>-66.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.31</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Bda San Luis, PR</td>\n",
       "      <td>NA-US-PR-BDA SAN LUIS</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61391</td>\n",
       "      <td>76166</td>\n",
       "      <td>UNIQUE</td>\n",
       "      <td>CINGULAR WIRELESS</td>\n",
       "      <td>TX</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>32.72</td>\n",
       "      <td>-97.31</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Cingular Wireless, TX</td>\n",
       "      <td>NA-US-TX-CINGULAR WIRELESS</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61392</td>\n",
       "      <td>76177</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>FORT WORTH</td>\n",
       "      <td>TX</td>\n",
       "      <td>PRIMARY</td>\n",
       "      <td>32.75</td>\n",
       "      <td>-97.33</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Fort Worth, TX</td>\n",
       "      <td>NA-US-TX-FORT WORTH</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2126</td>\n",
       "      <td>4053</td>\n",
       "      <td>122396986</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RecordNumber Zipcode ZipCodeType                 City State    LocationType  \\\n",
       "0            1     704    STANDARD          PARC PARQUE    PR  NOT ACCEPTABLE   \n",
       "1            2     704    STANDARD  PASEO COSTA DEL SUR    PR  NOT ACCEPTABLE   \n",
       "2           10     709    STANDARD         BDA SAN LUIS    PR  NOT ACCEPTABLE   \n",
       "3        61391   76166      UNIQUE    CINGULAR WIRELESS    TX  NOT ACCEPTABLE   \n",
       "4        61392   76177    STANDARD           FORT WORTH    TX         PRIMARY   \n",
       "\n",
       "     Lat    Long Xaxis  Yaxis Zaxis WorldRegion Country  \\\n",
       "0  17.96  -66.22  0.38  -0.87   0.3          NA      US   \n",
       "1  17.96  -66.22  0.38  -0.87   0.3          NA      US   \n",
       "2  18.14  -66.26  0.38  -0.86  0.31          NA      US   \n",
       "3  32.72  -97.31  -0.1  -0.83  0.54          NA      US   \n",
       "4  32.75  -97.33  -0.1  -0.83  0.54          NA      US   \n",
       "\n",
       "              LocationText                      Location Decommisioned  \\\n",
       "0          Parc Parque, PR          NA-US-PR-PARC PARQUE         FALSE   \n",
       "1  Paseo Costa Del Sur, PR  NA-US-PR-PASEO COSTA DEL SUR         FALSE   \n",
       "2         Bda San Luis, PR         NA-US-PR-BDA SAN LUIS         FALSE   \n",
       "3    Cingular Wireless, TX    NA-US-TX-CINGULAR WIRELESS         FALSE   \n",
       "4           Fort Worth, TX           NA-US-TX-FORT WORTH         FALSE   \n",
       "\n",
       "  TaxReturnsFiled EstimatedPopulation TotalWages Notes  \n",
       "0            None                None       None  None  \n",
       "1            None                None       None  None  \n",
       "2            None                None       None  None  \n",
       "3            None                None       None  None  \n",
       "4            2126                4053  122396986  None  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = spark.read.options(header = True, delimiter = ',').csv(\"../resources/zipcodes.csv\")\n",
    "df1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4853b777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Long: string (nullable = true)\n",
      " |-- Xaxis: string (nullable = true)\n",
      " |-- Yaxis: string (nullable = true)\n",
      " |-- Zaxis: string (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: string (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: string (nullable = true)\n",
      " |-- TotalWages: string (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()\n",
    "# so it did not print schema correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14dbb65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c189941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data-type does not look correct. lets change the data-type, and read schema again with corret data-type\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"RecordNumber\",IntegerType(),True) \\\n",
    "      .add(\"Zipcode\",IntegerType(),True) \\\n",
    "      .add(\"ZipCodeType\",StringType(),True) \\\n",
    "      .add(\"City\",StringType(),True) \\\n",
    "      .add(\"State\",StringType(),True) \\\n",
    "      .add(\"LocationType\",StringType(),True) \\\n",
    "      .add(\"Lat\",DoubleType(),True) \\\n",
    "      .add(\"Long\",DoubleType(),True) \\\n",
    "      .add(\"Xaxis\",IntegerType(),True) \\\n",
    "      .add(\"Yaxis\",DoubleType(),True) \\\n",
    "      .add(\"Zaxis\",DoubleType(),True) \\\n",
    "      .add(\"WorldRegion\",StringType(),True) \\\n",
    "      .add(\"Country\",StringType(),True) \\\n",
    "      .add(\"LocationText\",StringType(),True) \\\n",
    "      .add(\"Location\",StringType(),True) \\\n",
    "      .add(\"Decommisioned\",BooleanType(),True) \\\n",
    "      .add(\"TaxReturnsFiled\",StringType(),True) \\\n",
    "      .add(\"EstimatedPopulation\",IntegerType(),True) \\\n",
    "      .add(\"TotalWages\",IntegerType(),True) \\\n",
    "      .add(\"Notes\",StringType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d1d2a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: integer (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema = spark.read.options(header = True, delimiter = ',').format(\"csv\").schema(schema).load(\"../resources/zipcodes.csv\")\n",
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3d9ffd",
   "metadata": {},
   "source": [
    "## How to specify data types as you read in datasets.\n",
    "\n",
    "Some data types make it easier to infer schema (like tabular formats such as csv which we will show later). \n",
    "\n",
    "However you often have to set the schema yourself if you aren't dealing with a .read method that doesn't have inferSchema() built-in.\n",
    "\n",
    "Spark has all the tools you need for this, it just requires a very specific structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0730af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType,DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb17843",
   "metadata": {},
   "source": [
    "Next we need to create the list of Structure fields * :param name: string, name of the field. * :param dataType: :class:DataType of the field. * :param nullable: boolean, whether the field can be null (None) or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c845325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"name\", StringType(), True),\n",
    "               StructField(\"email\", StringType(), True),\n",
    "               StructField(\"city\", StringType(), True),\n",
    "               StructField(\"mac\", StringType(), True),\n",
    "               StructField(\"timestamp\", DateType(), True),\n",
    "               StructField(\"creditcard\", StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78baee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2295fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- mac: string (nullable = true)\n",
      " |-- timestamp: date (nullable = true)\n",
      " |-- creditcard: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people = spark.read.json(path+'people.json', schema=final_struc)\n",
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "092b5cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        math score|\n",
      "+-------+------------------+\n",
      "|  count|              1000|\n",
      "|   mean|            66.089|\n",
      "| stddev|15.163080096009454|\n",
      "|    min|                 0|\n",
      "|    max|               100|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neat \"describe\" function\n",
    "df.describe(['math score']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dec4b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+-------------+\n",
      "|summary|math score|reading score|writing score|\n",
      "+-------+----------+-------------+-------------+\n",
      "|  count|      1000|         1000|         1000|\n",
      "|    min|         0|           17|           10|\n",
      "|    25%|        57|           59|           57|\n",
      "|    75%|        77|           79|           79|\n",
      "|    max|       100|          100|          100|\n",
      "+-------+----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary function\n",
    "df.select(\"math score\", \"reading score\", \"writing score\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26330e2",
   "metadata": {},
   "source": [
    "## Specify data types as you read in datasets.\n",
    "\n",
    "Some data types make it easier to infer schema (like tabular formats such as csv which we will show later). \n",
    "\n",
    "However you often have to set the schema yourself if you aren't dealing with a .read method that doesn't have inferSchema() built-in.\n",
    "\n",
    "Spark has all the tools you need for this, it just requires a very specific structure.\n",
    "\n",
    "I've also included Spark's link to their latest list of data types for your reference in case you need it: https://spark.apache.org/docs/latest/sql-reference.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b25b8eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- mac: string (nullable = true)\n",
      " |-- timestamp: date (nullable = true)\n",
      " |-- creditcard: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e7fac4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>city</th>\n",
       "      <th>mac</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>creditcard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keeley Bosco</td>\n",
       "      <td>katlyn@jenkinsmaggio.net</td>\n",
       "      <td>Lake Gladysberg</td>\n",
       "      <td>08:fd:0b:cd:77:f7</td>\n",
       "      <td>2015-04-25</td>\n",
       "      <td>1228-1221-1221-1431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rubye Jerde</td>\n",
       "      <td>juvenal@johnston.name</td>\n",
       "      <td>None</td>\n",
       "      <td>90:4d:fa:42:63:a2</td>\n",
       "      <td>2015-04-25</td>\n",
       "      <td>1228-1221-1221-1431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miss Darian Breitenberg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>f9:0e:d3:40:cb:e9</td>\n",
       "      <td>2015-04-25</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name                     email             city  \\\n",
       "0                     None                      None             None   \n",
       "1             Keeley Bosco  katlyn@jenkinsmaggio.net  Lake Gladysberg   \n",
       "2              Rubye Jerde     juvenal@johnston.name             None   \n",
       "3  Miss Darian Breitenberg                      None             None   \n",
       "\n",
       "                 mac   timestamp           creditcard  \n",
       "0               None        None                 None  \n",
       "1  08:fd:0b:cd:77:f7  2015-04-25  1228-1221-1221-1431  \n",
       "2  90:4d:fa:42:63:a2  2015-04-25  1228-1221-1221-1431  \n",
       "3  f9:0e:d3:40:cb:e9  2015-04-25                 None  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74dba1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"name\", StringType(), True)]\n",
    "final_struc = StructType(fields=data_schema)\n",
    "people = spark.read.json(path+'people.json', schema=final_struc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41218ce",
   "metadata": {},
   "source": [
    "## Writing Data\n",
    "\n",
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c447a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * #StructField,StringType,IntegerType,StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "740dbdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the funky naming convention of the file in your output folder. There is no way to directly change this. \n",
    "df.write.mode(\"overwrite\").csv('write_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c5cfc",
   "metadata": {},
   "source": [
    "**Parquet files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d85f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe0a63",
   "metadata": {},
   "source": [
    "For those who got an error attempting to run the above code. Try this solution: https://stackoverflow.com/questions/59220832/unable-to-write-spark-dataframe-to-a-parquet-file-format-to-c-drive-in-pyspark\n",
    "\n",
    "#### Writting Partitioned Parquet Files\n",
    "\n",
    "Best practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb99695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").partitionBy(\"race/ethnicity\").parquet(\"partitioned_parquet_test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde84c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3022868",
   "metadata": {},
   "source": [
    "## Select Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d829c35",
   "metadata": {},
   "source": [
    "### **Order By**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d06aecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+\n",
      "|math score|reading score|writing score|\n",
      "+----------+-------------+-------------+\n",
      "|         0|           17|           10|\n",
      "|        30|           24|           15|\n",
      "|        28|           23|           19|\n",
      "|        30|           26|           22|\n",
      "|         8|           24|           23|\n",
      "+----------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fifa.select(['math score','reading score','writing score']).orderBy(\"writing score\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2cd5c",
   "metadata": {},
   "source": [
    "### **Order By Descending**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06a6c00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+\n",
      "|math score|reading score|writing score|\n",
      "+----------+-------------+-------------+\n",
      "|        99|          100|          100|\n",
      "|        88|           99|          100|\n",
      "|       100|          100|          100|\n",
      "|        96|          100|          100|\n",
      "|        85|           95|          100|\n",
      "+----------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['math score','reading score','writing score']).orderBy(df[\"writing score\"].desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1406427",
   "metadata": {},
   "source": [
    "### **LIKE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4a8bc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------+\n",
      "|gender|parental level of education|\n",
      "+------+---------------------------+\n",
      "|female|bachelor's degree          |\n",
      "|female|some college               |\n",
      "|female|master's degree            |\n",
      "|female|associate's degree         |\n",
      "|female|some college               |\n",
      "+------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('gender','parental level of education').where(df.gender.like(\"%fem%\")).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f6223",
   "metadata": {},
   "source": [
    "### **Substrings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f95fec71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|lunch       |substring|\n",
      "+------------+---------+\n",
      "|standard    |sta      |\n",
      "|standard    |sta      |\n",
      "|standard    |sta      |\n",
      "|free/reduced|fre      |\n",
      "|standard    |sta      |\n",
      "+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"lunch\",(df.lunch.substr(1,3)).alias('substring')).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b031da9",
   "metadata": {},
   "source": [
    "### **IS IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "448cf85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.select['parental level of education'].isin(\"some collegue\")].limit(4).toPandas()\n",
    "# ISIN(list)\n",
    "#df['Name','club','Nationality'].filter(\"Club IN ('FC Barcelona')\").limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c413e73",
   "metadata": {},
   "source": [
    "**Starts** with **Ends** with\n",
    "\n",
    "Search for a specific case - begins with \"x\" and ends with \"x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4a10486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fifa = spark.read.csv('../Datasets/fifa19.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae68c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_fifa.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "294dab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|Aggression|Stamina|\n",
      "+----------+-------+\n",
      "|        48|     72|\n",
      "|        63|     88|\n",
      "|        56|     81|\n",
      "|        38|     43|\n",
      "|        76|     90|\n",
      "+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fifa.select(['Aggression', 'Stamina']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffa9fc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+\n",
      "|summary|Aggression|Stamina|\n",
      "+-------+----------+-------+\n",
      "|  count|     18159|  18159|\n",
      "|    min|        11|     12|\n",
      "+-------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fifa.select(['Aggression', 'Stamina']).summary('count','min').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70e64aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------------+\n",
      "|     Name|age|        Club|\n",
      "+---------+---+------------+\n",
      "| L. Messi| 31|FC Barcelona|\n",
      "|L. Suárez| 31|FC Barcelona|\n",
      "+---------+---+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fifa.select([\"Name\", \"age\", \"Club\"]).where(df_fifa.Club.like(\"%celon%\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97fe1333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|               Photo|the last 4 charachter|\n",
      "+--------------------+---------------------+\n",
      "|https://cdn.sofif...|                 .png|\n",
      "|https://cdn.sofif...|                 .png|\n",
      "+--------------------+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT SUBSTRING ('what a wonderful DAY' from 2 for 6); -- hat a\n",
    "df_fifa.select(\"Photo\",df_fifa.Photo.substr(-4,5).alias('the last 4 charachter')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b006a2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Club</th>\n",
       "      <th>Nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L. Messi</td>\n",
       "      <td>FC Barcelona</td>\n",
       "      <td>Argentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L. Suárez</td>\n",
       "      <td>FC Barcelona</td>\n",
       "      <td>Uruguay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name          Club Nationality\n",
       "0   L. Messi  FC Barcelona   Argentina\n",
       "1  L. Suárez  FC Barcelona     Uruguay"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fifa['Name','Club','Nationality'].filter(\"Club IN ('FC Barcelona')\").limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3ab2953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|    Name|        Club|\n",
      "+--------+------------+\n",
      "|L. Messi|FC Barcelona|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fifa.select('Name', 'Club').where(df_fifa.Name.startswith(\"L\")) \\\n",
    "    .where(df_fifa.Name.endswith('i')).where(df_fifa.Club.like('%Barcelona')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7fc42d",
   "metadata": {},
   "source": [
    "### **Slicing**\n",
    "\n",
    "pyspark.sql.functions.slice(x, start, length)[source]\n",
    "Returns an array containing all the elements in x from index start (or starting from the end if start is negative) with the specified length.\n",
    "\n",
    "Note: indexing starts at 1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61dc3e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------------------+--------+-----------------------+----------+-------------+-------------+\n",
      "|gender|race/ethnicity|parental level of education|   lunch|test preparation course|math score|reading score|writing score|\n",
      "+------+--------------+---------------------------+--------+-----------------------+----------+-------------+-------------+\n",
      "|female|       group B|          bachelor's degree|standard|                   none|        72|           72|           74|\n",
      "+------+--------------+---------------------------+--------+-----------------------+----------+-------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## SLICING DataFrame, take n number of \"rows\"\n",
    "df.count()\n",
    "# get slice of data\n",
    "df1 = df.limit(100)\n",
    "df1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9093ea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----------+\n",
      "|    Name|        Club|Nationality|\n",
      "+--------+------------+-----------+\n",
      "|L. Messi|FC Barcelona|  Argentina|\n",
      "+--------+------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SLICING, take n number of \"colomns\"\n",
    "from pyspark.sql.functions import slice\n",
    "df2 = df_fifa.select('Name', 'Club', \"Nationality\")\n",
    "# OR\n",
    "#df_sel_col = df_fifa.select(df.columns[0:5])\n",
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a21ba",
   "metadata": {},
   "source": [
    "If you want to just slice your dataframe you can do this...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23dc038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting\n",
    "print('Starting row cound:',df.count())\n",
    "print('Starting column count:',len(df.columns))\n",
    "\n",
    "# Slice rows\n",
    "df2 = df.limit(300)\n",
    "print('Sliced row count:',df2.count())\n",
    "\n",
    "# Slice columns\n",
    "cols_list = df.columns[0:5]\n",
    "df3 = df.select(cols_list)\n",
    "print('Sliced column count:',len(df3.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974548f1",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "\n",
    "A large part of working with DataFrames is the ability to quickly filter out data based on conditions. Spark DataFrames are built on top of the Spark SQL platform, which means that is you already know SQL, you can quickly and easily grab that data using SQL commands, or using the DataFram methods (which is what we focus on in this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "161d0c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/18 14:19:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , ID, Name, Age, Photo, Nationality, Flag, Overall, Potential, Club, Club Logo, Value, Wage, Special, Preferred Foot, International Reputation, Weak Foot, Skill Moves, Work Rate, Body Type, Real Face, Position, Jersey Number, Joined, Loaned From, Contract Valid Until, Height, Weight, LS, ST, RS, LW, LF, CF, RF, RW, LAM, CAM, RAM, LM, LCM, CM, RCM, RM, LWB, LDM, CDM, RDM, RWB, LB, LCB, CB, RCB, RB, Crossing, Finishing, HeadingAccuracy, ShortPassing, Volleys, Dribbling, Curve, FKAccuracy, LongPassing, BallControl, Acceleration, SprintSpeed, Agility, Reactions, Balance, ShotPower, Jumping, Stamina, Strength, LongShots, Aggression, Interceptions, Positioning, Vision, Penalties, Composure, Marking, StandingTackle, SlidingTackle, GKDiving, GKHandling, GKKicking, GKPositioning, GKReflexes, Release Clause\n",
      " Schema: _c0, ID, Name, Age, Photo, Nationality, Flag, Overall, Potential, Club, Club Logo, Value, Wage, Special, Preferred Foot, International Reputation, Weak Foot, Skill Moves, Work Rate, Body Type, Real Face, Position, Jersey Number, Joined, Loaned From, Contract Valid Until, Height, Weight, LS, ST, RS, LW, LF, CF, RF, RW, LAM, CAM, RAM, LM, LCM, CM, RCM, RM, LWB, LDM, CDM, RDM, RWB, LB, LCB, CB, RCB, RB, Crossing, Finishing, HeadingAccuracy, ShortPassing, Volleys, Dribbling, Curve, FKAccuracy, LongPassing, BallControl, Acceleration, SprintSpeed, Agility, Reactions, Balance, ShotPower, Jumping, Stamina, Strength, LongShots, Aggression, Interceptions, Positioning, Vision, Penalties, Composure, Marking, StandingTackle, SlidingTackle, GKDiving, GKHandling, GKKicking, GKPositioning, GKReflexes, Release Clause\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/najmeh.foroozani/NotebookProject/pyspark/udemy-layla/Datasets/fifa19.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Photo</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Overall</th>\n",
       "      <th>Potential</th>\n",
       "      <th>Club</th>\n",
       "      <th>...</th>\n",
       "      <th>Composure</th>\n",
       "      <th>Marking</th>\n",
       "      <th>StandingTackle</th>\n",
       "      <th>SlidingTackle</th>\n",
       "      <th>GKDiving</th>\n",
       "      <th>GKHandling</th>\n",
       "      <th>GKKicking</th>\n",
       "      <th>GKPositioning</th>\n",
       "      <th>GKReflexes</th>\n",
       "      <th>Release Clause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>158023</td>\n",
       "      <td>L. Messi</td>\n",
       "      <td>31</td>\n",
       "      <td>https://cdn.sofifa.org/players/4/19/158023.png</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>https://cdn.sofifa.org/flags/52.png</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>FC Barcelona</td>\n",
       "      <td>...</td>\n",
       "      <td>96</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>€226.5M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _c0      ID      Name  Age                                           Photo  \\\n",
       "0    0  158023  L. Messi   31  https://cdn.sofifa.org/players/4/19/158023.png   \n",
       "\n",
       "  Nationality                                 Flag  Overall  Potential  \\\n",
       "0   Argentina  https://cdn.sofifa.org/flags/52.png       94         94   \n",
       "\n",
       "           Club  ... Composure Marking StandingTackle  SlidingTackle GKDiving  \\\n",
       "0  FC Barcelona  ...        96      33             28             26        6   \n",
       "\n",
       "   GKHandling  GKKicking  GKPositioning GKReflexes Release Clause  \n",
       "0          11         15             14          8        €226.5M  \n",
       "\n",
       "[1 rows x 89 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fifa.filter(\"Overall>50\").limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1a928bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158023</td>\n",
       "      <td>L. Messi</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Cristiano Ronaldo</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190871</td>\n",
       "      <td>Neymar Jr</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>193080</td>\n",
       "      <td>De Gea</td>\n",
       "      <td>Spain</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID               Name Nationality  Overall\n",
       "0  158023           L. Messi   Argentina       94\n",
       "1   20801  Cristiano Ronaldo    Portugal       94\n",
       "2  190871          Neymar Jr      Brazil       92\n",
       "3  193080             De Gea       Spain       91"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using SQL with .select()\n",
    "df_fifa.filter(\"Overall>50\").select(['ID','Name','Nationality','Overall']).limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1dbbb",
   "metadata": {},
   "source": [
    "### Collecting Results as Python Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1bf44873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting results as Python objects\n",
    "# you need the \".collect()\" call at the end to \"collect\" the results\n",
    "result = df_fifa.select(['Nationality','Name','Age','Overall']).filter(\"Overall>70\").orderBy(df_fifa[\"Overall\"].desc()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e70ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Player Over 70:  L. Messi\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Player Over 70: \",result[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a42ff1b",
   "metadata": {},
   "source": [
    "Rows can also be called to turn into dictionaries if needed, row as a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05036a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argentina\n",
      "L. Messi\n",
      "31\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "for item in result[0]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e0ba9",
   "metadata": {},
   "source": [
    "# <span class=\"burk\">SQL Options in Spark</span>\n",
    "\n",
    "### Spark SQL\n",
    "\n",
    "Spark TempView provides two functions that allow users to run SQL queries against a Spark DataFrame:\n",
    "\n",
    "createOrReplaceTempView: The lifetime of this temporary view is tied to the [[SparkSession]] that was used to create this Dataset. It creates (or replaces if that view name already exists) a lazily evaluated \"view\" that you can then use like a hive table in Spark SQL. It does not persist to memory unless you cache the dataset that underpins the view.\n",
    "\n",
    "createGlobalTempView: The lifetime of this temporary view is tied to this Spark application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9716e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = spark.read.csv(path+\"rec-crime-pfa.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b248fd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12 months ending</th>\n",
       "      <th>PFA</th>\n",
       "      <th>Region</th>\n",
       "      <th>Offence</th>\n",
       "      <th>Rolling year total number of offences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>All other theft offences</td>\n",
       "      <td>25959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Bicycle theft</td>\n",
       "      <td>3090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Criminal damage and arson</td>\n",
       "      <td>26202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  12 months ending                PFA      Region                    Offence  \\\n",
       "0       31/03/2003  Avon and Somerset  South West   All other theft offences   \n",
       "1       31/03/2003  Avon and Somerset  South West              Bicycle theft   \n",
       "2       31/03/2003  Avon and Somerset  South West  Criminal damage and arson   \n",
       "\n",
       "   Rolling year total number of offences  \n",
       "0                                  25959  \n",
       "1                                   3090  \n",
       "2                                  26202  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is way better\n",
    "crime.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa1ce5",
   "metadata": {},
   "source": [
    "So, in order for us to perform SQL calls off of this dataframe, we will need to rename any variables that have spaces in them. We will not be using the first variable so I'll leave that one as is, but we will be using the last variable, so I will go ahead and change that to Count so we can work with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "942ad666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 12 months ending: string (nullable = true)\n",
      " |-- PFA: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Offence: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = crime.withColumnRenamed('Rolling year total number of offences','Count') #.withColumn(\"12 months ending\", crime[\"12 months ending\"].cast(DateType())).\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d82c72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view of the dataframe\n",
    "df.createOrReplaceTempView(\"tempview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4f9243c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12 months ending</th>\n",
       "      <th>PFA</th>\n",
       "      <th>Region</th>\n",
       "      <th>Offence</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>All other theft offences</td>\n",
       "      <td>25959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  12 months ending                PFA      Region                   Offence  \\\n",
       "0       31/03/2003  Avon and Somerset  South West  All other theft offences   \n",
       "\n",
       "   Count  \n",
       "0  25959  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then Query the temp view\n",
    "spark.sql(\"SELECT * FROM tempview WHERE Count > 1000\").limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9531fa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>PFA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South West</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Region                PFA\n",
       "0  South West  Avon and Somerset"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or choose which vars you want\n",
    "spark.sql(\"SELECT Region, PFA FROM tempview WHERE Count > 1000\").limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5b14e622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12 months ending</th>\n",
       "      <th>PFA</th>\n",
       "      <th>Region</th>\n",
       "      <th>Offence</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>All other theft offences</td>\n",
       "      <td>25959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  12 months ending                PFA      Region                   Offence  \\\n",
       "0       31/03/2003  Avon and Somerset  South West  All other theft offences   \n",
       "\n",
       "   Count  \n",
       "0  25959  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also pass your query results to an object \n",
    "# (we don't need to use .collect() here)\n",
    "sql_results = spark.sql(\"SELECT * FROM tempview WHERE Count > 1000 AND Region='South West'\")\n",
    "sql_results.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "58c48260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fraud: CIFAS</td>\n",
       "      <td>7678981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Region    Total\n",
       "0  Fraud: CIFAS  7678981"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can even do aggregated \"group by\" calls like this\n",
    "spark.sql(\"SELECT Region, sum(Count) AS Total FROM tempview GROUP BY Region\").limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e088396",
   "metadata": {},
   "source": [
    "<div class=\"girk\">\n",
    "**Bonus!** <br>\n",
    "Not included in the lecture, but thought some of you may enjoy this. If you want to write more freeform style SQL you can enclose your query in triple quotes like this. Here I have shown an example using CTE which a more advanced SQL procedure. A common table expression (CTE) defines a temporary result set that a user can reference possibly multiple times within the scope of a SQL statement. A CTE is used mainly in a SELECT statement. Many people find this super useful. </div><i class=\"fa fa-lightbulb-o \"></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d5ec6d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1\n",
       "0  1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We could also do more complex SQL queries like CTE (not included )\n",
    "spark.sql(\"\"\"WITH t AS (\n",
    "    WITH tempview AS (SELECT 1)\n",
    "    SELECT * FROM tempview)\n",
    "SELECT * FROM t;\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbbcb39",
   "metadata": {},
   "source": [
    "**SQL Transformer**\n",
    "\n",
    "You also have the option to use the SQL transformer option where you can write freeform SQL scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "14b8a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to import SQL transformer\n",
    "from pyspark.ml.feature import SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4c9c95ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------------------+\n",
      "|              PFA|    Region|             Offence|\n",
      "+-----------------+----------+--------------------+\n",
      "|Avon and Somerset|South West|All other theft o...|\n",
      "|Avon and Somerset|South West|       Bicycle theft|\n",
      "|Avon and Somerset|South West|Criminal damage a...|\n",
      "+-----------------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Then we create an SQL call \n",
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT PFA,Region,Offence FROM __THIS__\") \n",
    "# And use it to transform our df object\n",
    "sqlTrans.transform(df).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9b1081a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.feature.SQLTransformer"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sqlTrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "67c89c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------------------+\n",
      "|              PFA|    Region|             Offence|\n",
      "+-----------------+----------+--------------------+\n",
      "|Avon and Somerset|South West|All other theft o...|\n",
      "|Avon and Somerset|South West|       Bicycle theft|\n",
      "+-----------------+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that \"__THIS__\" is a special word and cannot be change to __THAT__ for example\n",
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT PFA,Region,Offence FROM __THIS__\") \n",
    "# And use it to transform our df object\n",
    "sqlTrans.transform(df).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c189ab04",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SQLTransformer' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pw/8mmy16sd6rz892yd5p157q0mvj2klr/T/ipykernel_2789/2169131779.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Also Note that a call like this won't work...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mSQLTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SELECT PFA,Region,Offence FROM __THIS__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SQLTransformer' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "# Also Note that a call like this won't work...\n",
    "SQLTransformer(statement=\"SELECT PFA,Region,Offence FROM __THIS__\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a466bd63",
   "metadata": {},
   "source": [
    "**Now how about a group by call**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "15d42c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             Offence|   Total|\n",
      "+--------------------+--------+\n",
      "|Public order offe...|10925676|\n",
      "|       Bicycle theft| 5297006|\n",
      "+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Note that this call will not work on the original dataframe \"crime\" when the variable \"Count\" is a string\n",
    "\n",
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT Offence, SUM(Count) as Total FROM __THIS__ GROUP BY Offence\") \n",
    "sqlTrans.transform(df).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9bbb660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|              PFA|             Offence|\n",
      "+-----------------+--------------------+\n",
      "|Avon and Somerset|All other theft o...|\n",
      "|Avon and Somerset|       Bicycle theft|\n",
      "+-----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT PFA,Offence FROM __THIS__ WHERE Count > 1000\") \n",
    "sqlTrans.transform(df).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7e08b483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|              PFA|             Offence|\n",
      "+-----------------+--------------------+\n",
      "|Avon and Somerset|All other theft o...|\n",
      "|Avon and Somerset|       Bicycle theft|\n",
      "+-----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can also, of course, read the output into a dataframe\n",
    "result = sqlTrans.transform(df)\n",
    "result.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8ea31",
   "metadata": {},
   "source": [
    "## SQL Options within regular PySpark calls\n",
    "\n",
    "### The expr function in PySparks SQL Function Library\n",
    "\n",
    "You can also use the expr function within the pyspark.sql.functions library coupled with either PySpark's withColumn function or the select function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b7ebeb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to read in the library\n",
    "from pyspark.sql.functions import expr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d00f50",
   "metadata": {},
   "source": [
    "Let's add a percent column to the dataframe. To do this, first we need to get the total number of rows in the dataframe (we can't soft this unfortunatly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cd2a8dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    Total|\n",
      "+---------+\n",
      "|244720928|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT SUM(Count) as Total FROM __THIS__\") \n",
    "sqlTrans.transform(df).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1eb465cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "|12 months ending|              PFA|    Region|             Offence|Count|percent|\n",
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "|      31/03/2003|Avon and Somerset|South West|All other theft o...|25959|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|       Bicycle theft| 3090|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Criminal damage a...|26202|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|Death or serious ...|    2|    0.0|\n",
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We could add a percent column to our df \n",
    "# that shows the offence %\n",
    "# with the \"withColumn\" command\n",
    "df.withColumn(\"percent\",expr(\"round((count/244720928)*100,2)\")).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f1fac7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "|12 months ending|              PFA|    Region|             Offence|Count|percent|\n",
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "|      31/03/2003|Avon and Somerset|South West|All other theft o...|25959|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|       Bicycle theft| 3090|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Criminal damage a...|26202|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|Death or serious ...|    2|    0.0|\n",
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same thing with the \"select\" command\n",
    "df.select(\"*\",expr(\"round((count/244720928)*100,2) AS percent\")).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893321f",
   "metadata": {},
   "source": [
    "### PySparks selectExpr function\n",
    "\n",
    "Very similar idea here but slightly different syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "80b3dcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "|12 months ending|              PFA|    Region|             Offence|Count|percent|\n",
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "|      31/03/2003|Avon and Somerset|South West|All other theft o...|25959|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|       Bicycle theft| 3090|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Criminal damage a...|26202|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|Death or serious ...|    2|    0.0|\n",
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"*\",\"round((count/244720928)*100,2) AS percent\").filter(\"Region ='South West'\").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a8ae5",
   "metadata": {},
   "source": [
    "# <span class=\"burk\">GroupBy and Aggregate Functions</span>\n",
    "\n",
    "Let's learn how to use GroupBy and Aggregate methods on a DataFrame. GroupBy allows you to group rows together based off some column value, for example, you could group together sales data by the day the sale occured, or group repeast customer data based off the name of the customer. Once you've performed the GroupBy operation you can use an aggregate function off that data. An aggregate function aggregates multiple rows of data into a single output, such as taking the sum of inputs, or counting the number of inputs.\n",
    "\n",
    "Let's see some examples on an example dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "73d1c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by reading in a basic csv dataset\n",
    "# Let Spark know about the header and infer the Schema types!\n",
    "\n",
    "#Some csv data\n",
    "airbnb = spark.read.csv('../Datasets/nyc_air_bnb.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "51f2a729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- reviews_per_month: integer (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>2787</td>\n",
       "      <td>John</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>40.64749</td>\n",
       "      <td>-73.97237</td>\n",
       "      <td>Private room</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-10-19</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                name host_id host_name  \\\n",
       "0  2539  Clean & quiet apt home by the park    2787      John   \n",
       "\n",
       "  neighbourhood_group neighbourhood  latitude  longitude     room_type  price  \\\n",
       "0            Brooklyn    Kensington  40.64749  -73.97237  Private room    149   \n",
       "\n",
       "   minimum_nights  number_of_reviews last_review  reviews_per_month  \\\n",
       "0               1                  9  2018-10-19                  0   \n",
       "\n",
       "   calculated_host_listings_count  availability_365  \n",
       "0                               6               365  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice here that some of the columns that are obviously numeric have been incorrectly identified as \"strings\". Let's edit that. Otherwise we cannot aggregate any of the numeric columns.\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = airbnb.withColumn(\"price\", airbnb[\"price\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"minimum_nights\", airbnb[\"minimum_nights\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"number_of_reviews\", airbnb[\"number_of_reviews\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"reviews_per_month\", airbnb[\"reviews_per_month\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"calculated_host_listings_count\", airbnb[\"calculated_host_listings_count\"].cast(IntegerType()))\n",
    "#QA\n",
    "print(df.printSchema())\n",
    "df.limit(1).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a80f23",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregate Functions\n",
    "\n",
    "Let's learn how to use GroupBy and Aggregate methods on a DataFrame. These two commands go hand in hand many times in PySpark. ACtually in order to use the GroupBy command, you have to also tell Spark what numeric aggregate you want to learn about. For example, count, average or min/max. \n",
    "\n",
    "GroupBy allows you to group rows together based off some column value, for example, you could group together sales data by the day the sale occured, or group repeat customer data based off the name of the customer. Once you've performed the GroupBy operation you can use an aggregate function off that data. An aggregate function aggregates multiple rows of data into a single output, such as taking the sum of inputs, or counting the number of inputs.\n",
    "\n",
    "You can also use the aggreate function independently as well to learn about overall statistics of your dataframe too which we will see in some of our examples. \n",
    "\n",
    "So let's dig in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2a5ae1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|neighbourhood_group|count|\n",
      "+-------------------+-----+\n",
      "|         Douglaston|    1|\n",
      "|             Queens| 5630|\n",
      "|              Nadia|    1|\n",
      "|            Midtown|    4|\n",
      "|    Jackson Heights|    2|\n",
      "|     Hell's Kitchen|    7|\n",
      "|  Greenwich Village|    2|\n",
      "+-------------------+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For example we may be interested to see how many listings there were per neighbourhood group. \n",
    "# Groupby Function with count (you can also use sum, min, max)\n",
    "df.groupBy(\"neighbourhood_group\").count().show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1c506c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|neighbourhood_group|       avg(price)|\n",
      "+-------------------+-----------------+\n",
      "|         Douglaston|              1.0|\n",
      "|             Queens|99.57690941385435|\n",
      "|              Nadia|             null|\n",
      "+-------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Then you can add the following aggregate functions: mean, count, min, max, sum\n",
    "# Like this for example\n",
    "df.groupBy(\"neighbourhood_group\").mean(\"price\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5eb49d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|neighbourhood|avg(price)|\n",
      "+-------------+----------+\n",
      "|       Corona| 59.171875|\n",
      "| Richmondtown|      78.0|\n",
      "| Prince's Bay|     409.5|\n",
      "+-------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is another way of doing the above but I don't recommend it\n",
    "# because you can only do one var at a time\n",
    "df.groupBy(\"neighbourhood\").agg({'price':'mean'}).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e2f904e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+\n",
      "|neighbourhood|Min Price|Max Price|\n",
      "+-------------+---------+---------+\n",
      "|       Corona|       23|      359|\n",
      "| Richmondtown|       78|       78|\n",
      "| Prince's Bay|       85|     1250|\n",
      "|  Westerleigh|       40|      103|\n",
      "|   Mill Basin|       85|      299|\n",
      "+-------------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This method is way more versatile\n",
    "# Allows you to call on more than one aggregate function at a time\n",
    "# It's my fav for this reason!\n",
    "from pyspark.sql.functions import *\n",
    "df.groupBy(\"neighbourhood\").agg(min(df.price).alias(\"Min Price\"),max(df.price).alias(\"Max Price\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "76270546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>49079</td>\n",
       "      <td>49047</td>\n",
       "      <td>48894</td>\n",
       "      <td>48873</td>\n",
       "      <td>48894</td>\n",
       "      <td>48894</td>\n",
       "      <td>48894</td>\n",
       "      <td>48894</td>\n",
       "      <td>48894</td>\n",
       "      <td>48887</td>\n",
       "      <td>48891</td>\n",
       "      <td>48738</td>\n",
       "      <td>38845</td>\n",
       "      <td>38858</td>\n",
       "      <td>48891</td>\n",
       "      <td>48737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>min</td>\n",
       "      <td>12 mins Manhattan\"</td>\n",
       "      <td>1 Bed Apt in Utopic Williamsburg</td>\n",
       "      <td>Heart of Greenwich Village\"</td>\n",
       "      <td>very clean studio app\"</td>\n",
       "      <td>194716858</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.72247</td>\n",
       "      <td>-73.71299</td>\n",
       "      <td>-73.90783</td>\n",
       "      <td>-74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.94134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25%</td>\n",
       "      <td>9471893.0</td>\n",
       "      <td>2.4544724E7</td>\n",
       "      <td>7797690.0</td>\n",
       "      <td>475.0</td>\n",
       "      <td>1.94716858E8</td>\n",
       "      <td>40.68771</td>\n",
       "      <td>40.68981</td>\n",
       "      <td>-73.98309</td>\n",
       "      <td>56.0</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75%</td>\n",
       "      <td>2.9152899E7</td>\n",
       "      <td>1.74786681E8</td>\n",
       "      <td>1.07434423E8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.97400421E8</td>\n",
       "      <td>40.78304</td>\n",
       "      <td>40.76299</td>\n",
       "      <td>-73.93638</td>\n",
       "      <td>145.0</td>\n",
       "      <td>175</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>獨一無二的紐約閣樓\"</td>\n",
       "      <td>ﾏﾝﾊｯﾀﾝ､駅から徒歩4分でどこに行くのにも便利な場所!女性の方希望,ｷﾚｲなお部屋｡</td>\n",
       "      <td>呈刚</td>\n",
       "      <td>현선</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>West Village</td>\n",
       "      <td>Shared room</td>\n",
       "      <td>Shared room</td>\n",
       "      <td>10000</td>\n",
       "      <td>1250</td>\n",
       "      <td>629</td>\n",
       "      <td>9.66</td>\n",
       "      <td>58</td>\n",
       "      <td>365</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                   id                                          name  \\\n",
       "0   count                49079                                         49047   \n",
       "1     min   12 mins Manhattan\"             1 Bed Apt in Utopic Williamsburg    \n",
       "2     25%            9471893.0                                   2.4544724E7   \n",
       "3     75%          2.9152899E7                                  1.74786681E8   \n",
       "4     max           獨一無二的紐約閣樓\"  ﾏﾝﾊｯﾀﾝ､駅から徒歩4分でどこに行くのにも便利な場所!女性の方希望,ｷﾚｲなお部屋｡   \n",
       "\n",
       "                        host_id                host_name neighbourhood_group  \\\n",
       "0                         48894                    48873               48894   \n",
       "1   Heart of Greenwich Village\"   very clean studio app\"           194716858   \n",
       "2                     7797690.0                    475.0        1.94716858E8   \n",
       "3                  1.07434423E8                      NaN        1.97400421E8   \n",
       "4                            呈刚                       현선            Woodside   \n",
       "\n",
       "  neighbourhood      latitude    longitude    room_type  price minimum_nights  \\\n",
       "0         48894         48894        48894        48894  48887          48891   \n",
       "1             2     -73.72247    -73.71299    -73.90783    -74              0   \n",
       "2      40.68771      40.68981    -73.98309         56.0     69              1   \n",
       "3      40.78304      40.76299    -73.93638        145.0    175              5   \n",
       "4      Woodside  West Village  Shared room  Shared room  10000           1250   \n",
       "\n",
       "  number_of_reviews last_review reviews_per_month  \\\n",
       "0             48738       38845             38858   \n",
       "1                 0   -73.94134                 0   \n",
       "2                 1        0.76                 0   \n",
       "3                23        3.24                 2   \n",
       "4               629        9.66                58   \n",
       "\n",
       "  calculated_host_listings_count availability_365  \n",
       "0                          48891            48737  \n",
       "1                              0                0  \n",
       "2                              1                0  \n",
       "3                              2              226  \n",
       "4                            365              365  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is also a pretty neat function you can use:\n",
    "summary = df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\")\n",
    "summary.toPandas()\n",
    "# But be careful because it'll perform this operation on your whole df!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "75eb559b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>48887</td>\n",
       "      <td>48891</td>\n",
       "      <td>48738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>min</td>\n",
       "      <td>-74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max</td>\n",
       "      <td>10000</td>\n",
       "      <td>1250</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary  price minimum_nights number_of_reviews\n",
       "0   count  48887          48891             48738\n",
       "1     min    -74              0                 0\n",
       "2     max  10000           1250               629"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eh that was ugly!\n",
    "# To do a summary for specific columns first select them:\n",
    "# limit_summary = df.select(\"price\",\"minimum_nights\",\"number_of_reviews\",\"last_review\",\"reviews_per_month\",\"calculated_host_listings_count\",\"availability_365\").summary(\"count\",\"min\",\"max\")\n",
    "limit_summary = df.select(\"price\",\"minimum_nights\",\"number_of_reviews\").summary(\"count\",\"min\",\"max\")\n",
    "limit_summary.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943cd8b",
   "metadata": {},
   "source": [
    "### Aggregate on the entire DataFrame without groups (shorthand for df.groupBy.agg()).\n",
    "\n",
    "This is great, but what if we wanted the overall summary metrics like average and counts for more than one variable and without a groupBy variable? We could do this using the pyspark.sql functions library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c647868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|Min Price|Max Price|\n",
      "+---------+---------+\n",
      "|      -74|    10000|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate!\n",
    "# agg(*exprs)\n",
    "# Aggregate on the entire DataFrame without groups (shorthand for df.groupBy.agg()).\n",
    "# available agg functions: min, max, count, countDistinct, approx_count_distinct\n",
    "# df.agg.(covar_pop(col1, col2)) Returns a new Column for the population covariance of col1 and col2\n",
    "# df.agg.(covar_samp(col1, col2)) Returns a new Column for the sample covariance of col1 and col2.\n",
    "# df.agg(corr(col1, col2)) Returns a new Column for the Pearson Correlation Coefficient for col1 and col2.\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df.agg(min(df.price).alias(\"Min Price\"),max(df.price).alias(\"Max Price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ab29f80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 143:==================================>                  (130 + 9) / 200]\r",
      "\r",
      "[Stage 143:==============================================>      (177 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------------------+\n",
      "|CountD|        avg(price)|stddev_samp(price)|\n",
      "+------+------------------+------------------+\n",
      "|    77|152.22298361527604|238.54146688839478|\n",
      "+------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# There is also this method which is pretty similar\n",
    "df.select(countDistinct(\"neighbourhood_group\").alias('CountD'),avg('price'),stddev(\"price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f1e97333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Max Reviews|\n",
      "+-----------+\n",
      "|        629|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You could also write the syntax like this....\n",
    "# But keep in mind with this method that you can only do one variable at a time (bummer)\n",
    "# Again I don't recommend this!\n",
    "# Max sales across everything\n",
    "df.agg({'number_of_reviews':'max'}).withColumnRenamed(\"max(number_of_reviews)\", \"Max Reviews\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545992d7",
   "metadata": {},
   "source": [
    "### Pivot Function\n",
    "Provides a two way table and must be used in conjunction with groupBy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1ef2f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 148:=======================>                              (88 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+\n",
      "|room_type|Queens|Brooklyn|\n",
      "+---------+------+--------+\n",
      "|       51|  null|    null|\n",
      "|      205|  null|    null|\n",
      "|       54|  null|    null|\n",
      "+---------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 148:========================================>            (154 + 8) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Pivot Function\n",
    "# pivot(pivot_col, values=None)\n",
    "df.groupBy(\"room_type\").pivot(\"neighbourhood_group\", [\"Queens\", \"Brooklyn\"]).count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dd3102fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------+\n",
      "|  room_type|Queens|Brooklyn|\n",
      "+-----------+------+--------+\n",
      "|Shared room|   198|     413|\n",
      "+-----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can also filter your results if you need to\n",
    "# We some invalid data in the above output\n",
    "# So we could select only the \"Share room\" types if we wanted to\n",
    "df.filter(\"room_type='Shared room'\").groupBy(\"room_type\") \\\n",
    "    .pivot(\"neighbourhood_group\", [\"Queens\", \"Brooklyn\"]).count().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd61c66",
   "metadata": {},
   "source": [
    "### Comine all three!\n",
    "It is also possible to combine all three method into one call: GroupBy, Pivot and Agg like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8f6b3da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>Queens_Min Price</th>\n",
       "      <th>Queens_Max Price</th>\n",
       "      <th>Brooklyn_Min Price</th>\n",
       "      <th>Brooklyn_Max Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Corona</td>\n",
       "      <td>23.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prince's Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Richmondtown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mill Basin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "      <td>299.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Westerleigh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>40.69383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Morningside Heights</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Greenpoint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Elmhurst</td>\n",
       "      <td>15.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>Little Italy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>383 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           neighbourhood  Queens_Min Price  Queens_Max Price  \\\n",
       "0                 Corona              23.0             359.0   \n",
       "1           Prince's Bay               NaN               NaN   \n",
       "2           Richmondtown               NaN               NaN   \n",
       "3             Mill Basin               NaN               NaN   \n",
       "4            Westerleigh               NaN               NaN   \n",
       "..                   ...               ...               ...   \n",
       "378             40.69383               NaN               NaN   \n",
       "379  Morningside Heights               NaN               NaN   \n",
       "380           Greenpoint               NaN               NaN   \n",
       "381             Elmhurst              15.0             443.0   \n",
       "382         Little Italy               NaN               NaN   \n",
       "\n",
       "     Brooklyn_Min Price  Brooklyn_Max Price  \n",
       "0                   NaN                 NaN  \n",
       "1                   NaN                 NaN  \n",
       "2                   NaN                 NaN  \n",
       "3                  85.0               299.0  \n",
       "4                   NaN                 NaN  \n",
       "..                  ...                 ...  \n",
       "378                 NaN                 NaN  \n",
       "379                 NaN                 NaN  \n",
       "380                 0.0             10000.0  \n",
       "381                 NaN                 NaN  \n",
       "382                 NaN                 NaN  \n",
       "\n",
       "[383 rows x 5 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "df.groupBy(\"neighbourhood\").pivot(\"neighbourhood_group\", [\"Queens\", \"Brooklyn\"]) \\\n",
    "    .agg(min(df.price).alias(\"Min Price\"),max(df.price).alias(\"Max Price\")).toPandas()#.show()\n",
    "# Note The toPandas() method should only be used if the resulting Pandas’s DataFrame is expected to be small, \n",
    "# as all the data is loaded into the driver’s memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b55d78",
   "metadata": {},
   "source": [
    "# <span class=\"burk\">Joining and Appending DataFrames in PySpark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06767bb0",
   "metadata": {},
   "source": [
    "## Generate play data\n",
    "\n",
    "First some play data to help us grasp some concepts. Let's create a database that has two tables. \n",
    "\n",
    "**Key Terms**\n",
    " - **omnivore**: an animal which is able to consume both plants (like a herbivore) and meat (like a carnivore)\n",
    " - **herbivore**: any animal that eats only vegetation (i.e. that eats no meat)\n",
    " - **carnivore**: any animal that eats meat as the main part of its diet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cd3f16ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant eaters (herbivores)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 174:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-----------+\n",
      "|       name| id|eats_plants|\n",
      "+-----------+---+-----------+\n",
      "|      koala|  1|        yes|\n",
      "|caterpillar|  2|        yes|\n",
      "|       deer|  3|        yes|\n",
      "|      human|  4|        yes|\n",
      "+-----------+---+-----------+\n",
      "\n",
      "None\n",
      "Meat eaters (carnivores)\n",
      "+-----+---+---------+\n",
      "| name| id|eats_meat|\n",
      "+-----+---+---------+\n",
      "|shark|  5|      yes|\n",
      "| lion|  6|      yes|\n",
      "|tiger|  7|      yes|\n",
      "|human|  4|      yes|\n",
      "+-----+---+---------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "valuesP = [('koala',1,'yes'),('caterpillar',2,'yes'),('deer',3,'yes'),('human',4,'yes')]\n",
    "eats_plants = spark.createDataFrame(valuesP,['name','id','eats_plants'])\n",
    "\n",
    "valuesM = [('shark',5,'yes'),('lion',6,'yes'),('tiger',7,'yes'),('human',4,'yes')]\n",
    "eats_meat = spark.createDataFrame(valuesM,['name','id','eats_meat'])\n",
    "\n",
    "print(\"Plant eaters (herbivores)\")\n",
    "print(eats_plants.show())\n",
    "print(\"Meat eaters (carnivores)\")\n",
    "print(eats_meat.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f89021",
   "metadata": {},
   "source": [
    "### Appends\n",
    "Appending \"appends\" two dataframes together that have the exact same variables. You can think of it like stacking two or more blocks ON TOP of each other. To demonstrate this, we will simply join the same dataframe to itself since we don't really have a good use case for this. But hopefully this will help you imagine what to do.\n",
    "\n",
    "A common usecase would be joining the same table of infomation from one year to another year (i.e. 2012 + 2013 + ...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c451f883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('eats_plants df Counts:', 4, 3)\n",
      "('df_concat Counts:', 8, 3)\n",
      "+-----------+---+-----------+\n",
      "|       name| id|eats_plants|\n",
      "+-----------+---+-----------+\n",
      "|      koala|  1|        yes|\n",
      "|caterpillar|  2|        yes|\n",
      "|       deer|  3|        yes|\n",
      "|      human|  4|        yes|\n",
      "+-----------+---+-----------+\n",
      "\n",
      "None\n",
      "+-----------+---+-----------+\n",
      "|       name| id|eats_plants|\n",
      "+-----------+---+-----------+\n",
      "|      koala|  1|        yes|\n",
      "|caterpillar|  2|        yes|\n",
      "|       deer|  3|        yes|\n",
      "|      human|  4|        yes|\n",
      "|      koala|  1|        yes|\n",
      "+-----------+---+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# So first replicate table and call it new_df\n",
    "new_df = eats_plants\n",
    "# Then append using the union function\n",
    "# this naming convention can be tricky to grasp for SQL enthusiasts \n",
    "# Where union just mean join\n",
    "df_concat = eats_plants.union(new_df)\n",
    "# We will test to see if this worked by getting before and after row counts\n",
    "print((\"eats_plants df Counts:\", eats_plants.count(), len(eats_plants.columns)))\n",
    "print((\"df_concat Counts:\", df_concat.count(), len(df_concat.columns)))\n",
    "print(eats_plants.show(5))\n",
    "print(df_concat.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e51f1",
   "metadata": {},
   "source": [
    "## Inner Joins!\n",
    "\n",
    "Inner joins get us ONLY the values that appear in BOTH tables we are joining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1fd25327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Join Example\n",
      "+-----+---+-----------+---------+\n",
      "| name| id|eats_plants|eats_meat|\n",
      "+-----+---+-----------+---------+\n",
      "|human|  4|        yes|      yes|\n",
      "+-----+---+-----------+---------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inner_join = eats_plants.join(eats_meat, [\"name\",\"id\"],\"inner\")\n",
    "print(\"Inner Join Example\")\n",
    "print(inner_join.show())\n",
    "# So this is the only name that appears in BOTH dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca987965",
   "metadata": {},
   "source": [
    "## Left Joins\n",
    "\n",
    "Left joins get us the values that appear in the left table and nothing additional from the right table except for its columns. A quick quality check we could do would be to make sure that the human column has the value \"yes\" for both eats_plants and eats_meat columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b3f3d08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Join Example\n",
      "+-----------+---+-----------+---------+\n",
      "|       name| id|eats_plants|eats_meat|\n",
      "+-----------+---+-----------+---------+\n",
      "|       deer|  3|        yes|     null|\n",
      "|      human|  4|        yes|      yes|\n",
      "|      koala|  1|        yes|     null|\n",
      "|caterpillar|  2|        yes|     null|\n",
      "+-----------+---+-----------+---------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "left_join = eats_plants.join(eats_meat, [\"name\",\"id\"], how='left') # Could also use 'left_outer'\n",
    "print(\"Left Join Example\")\n",
    "print(left_join.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6446f82",
   "metadata": {},
   "source": [
    "## Conditional Joins\n",
    "\n",
    "Conditional joins have some additional logic that was not encompassed in the underlying join. For example, if we wanted to get all the values that appear in the left, **except** for those values that appear in BOTH tables, we could do this. Notice how human is left out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "53164c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Left Join\n",
      "+-----------+---+-----------+---------+\n",
      "|       name| id|eats_plants|eats_meat|\n",
      "+-----------+---+-----------+---------+\n",
      "|       deer|  3|        yes|     null|\n",
      "|      koala|  1|        yes|     null|\n",
      "|caterpillar|  2|        yes|     null|\n",
      "+-----------+---+-----------+---------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "conditional_join = eats_plants.join(eats_meat, [\"name\",\"id\"], how='left').filter(eats_meat.name.isNull())\n",
    "print(\"Conditional Left Join\")\n",
    "print(conditional_join.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ef08a",
   "metadata": {},
   "source": [
    "## Right Join\n",
    "\n",
    "A right join gets you the values that appear in the right table but not in the left. It also brings it's columns over of course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "50973367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right Join\n",
      "+-----+---+-----------+---------+\n",
      "| name| id|eats_plants|eats_meat|\n",
      "+-----+---+-----------+---------+\n",
      "|shark|  5|       null|      yes|\n",
      "|human|  4|        yes|      yes|\n",
      "|tiger|  7|       null|      yes|\n",
      "| lion|  6|       null|      yes|\n",
      "+-----+---+-----------+---------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "right_join = eats_plants.join(eats_meat,  [\"name\",\"id\"],how='right') # Could also use 'right_outer'\n",
    "print(\"Right Join\")\n",
    "print(right_join.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da79dd8",
   "metadata": {},
   "source": [
    "## Full Outer Joins\n",
    "\n",
    "Full outer joins will get all values from both tables, but notice that if there is a column that is common in both tables (ie. id and name in this case) that the join will take the value of the left table (see human id is p4 and not m4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "985546c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Outer Join\n",
      "+-----------+---+-----------+---------+\n",
      "|       name| id|eats_plants|eats_meat|\n",
      "+-----------+---+-----------+---------+\n",
      "|       deer|  3|        yes|     null|\n",
      "|      shark|  5|       null|      yes|\n",
      "|      human|  4|        yes|      yes|\n",
      "|      tiger|  7|       null|      yes|\n",
      "|       lion|  6|       null|      yes|\n",
      "|      koala|  1|        yes|     null|\n",
      "|caterpillar|  2|        yes|     null|\n",
      "+-----------+---+-----------+---------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "full_outer_join = eats_plants.join(eats_meat, [\"name\",\"id\"],how='full') # Could also use 'full_outer'\n",
    "print(\"Full Outer Join\")\n",
    "print(full_outer_join.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d078875b",
   "metadata": {},
   "source": [
    "## Alright now let's try with REAL data\n",
    "\n",
    "Thinking about how to join your data in real life will not be as easy as the above. You need to consider multiple aspects as you join tables in real life and ALWAYS conduct sanity checks to make sure you did it correctly. Let's look at an example below with real data.\n",
    "\n",
    "#### First, let's read in the datasets we will be working with\n",
    "\n",
    "Here is a neat function that will read in all the csv files from a directory (folder) in one shot and returns a separate dataframe for each dataset in the directory using the same naming convention. This is super useful if you have a large set of files and don't feel like writing a separate line for each dataset in the directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "41e43a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full list of dfs:\n",
      "['subjects', 'subject_memberships', 'rooms', 'schedules', 'sections', 'courses', 'course_offerings', 'instructors', 'teachings', 'grade_distributions']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"../Datasets/uw-madison-courses/\"\n",
    "\n",
    "df_list = []\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        filename_list = filename.split(\".\") #separate path from .csv\n",
    "        df_name = filename_list[0]\n",
    "        df = spark.read.csv(path+filename,inferSchema=True,header=True)\n",
    "        df.name = df_name\n",
    "        df_list.append(df_name)\n",
    "        exec(df_name + ' = df')\n",
    "        \n",
    "# QA\n",
    "print(\"Full list of dfs:\")\n",
    "print(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c04bc",
   "metadata": {},
   "source": [
    "## About this database\n",
    "\n",
    "You will notice that there are several tables in the uw-madision-courses folder that were read in above. This database will let you get a chance to practice your own custom joins and learn how the relationships between a real database work. Sometimes we don't know how they are related and we need to figure it out! I'll save that for the HW since we will be using the same database :) So I just wanted to introduce the database to you quickly here first. \n",
    "\n",
    "For this lecture, we will focus on the 4 datasets below and save the rest for the HW. Here is a look at some of the important variables we will be using to join our tables:\n",
    "\n",
    " - **course_offerings:** uuid, course_uuid, term_code, name\n",
    " - **instructors:** id, name\n",
    " - **sections:** uuid, course_offering_uuid,room_uuid, schedule_uuid\n",
    " - **teachings:** instructor_id, section_uuid\n",
    " \n",
    " **Source:** https://www.kaggle.com/Madgrades/uw-madison-courses\n",
    " \n",
    "Let's pretend that I am a student interested in seeing what courses are available. I suppose I would start by look at the course offerings table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6acde0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>course_uuid</th>\n",
       "      <th>term_code</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>344b3ebe-da7e-314c-83ed-9425269695fd</td>\n",
       "      <td>a3e3e1c3-543d-3bb5-ae65-5f2aec4ad1de</td>\n",
       "      <td>1092</td>\n",
       "      <td>Cooperative Education Prog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f718e6cd-33f0-3c14-a9a6-834d9c3610a8</td>\n",
       "      <td>a3e3e1c3-543d-3bb5-ae65-5f2aec4ad1de</td>\n",
       "      <td>1082</td>\n",
       "      <td>Cooperative Education Prog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ea3b717c-d66b-30dc-8b37-964d9688295f</td>\n",
       "      <td>a3e3e1c3-543d-3bb5-ae65-5f2aec4ad1de</td>\n",
       "      <td>1172</td>\n",
       "      <td>Cooperative Education Prog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>075da420-5f49-3dd0-93df-13e3c152e1b1</td>\n",
       "      <td>a3e3e1c3-543d-3bb5-ae65-5f2aec4ad1de</td>\n",
       "      <td>1114</td>\n",
       "      <td>Cooperative Education Prog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid                           course_uuid  \\\n",
       "0  344b3ebe-da7e-314c-83ed-9425269695fd  a3e3e1c3-543d-3bb5-ae65-5f2aec4ad1de   \n",
       "1  f718e6cd-33f0-3c14-a9a6-834d9c3610a8  a3e3e1c3-543d-3bb5-ae65-5f2aec4ad1de   \n",
       "2  ea3b717c-d66b-30dc-8b37-964d9688295f  a3e3e1c3-543d-3bb5-ae65-5f2aec4ad1de   \n",
       "3  075da420-5f49-3dd0-93df-13e3c152e1b1  a3e3e1c3-543d-3bb5-ae65-5f2aec4ad1de   \n",
       "\n",
       "   term_code                        name  \n",
       "0       1092  Cooperative Education Prog  \n",
       "1       1082  Cooperative Education Prog  \n",
       "2       1172  Cooperative Education Prog  \n",
       "3       1114  Cooperative Education Prog  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the data\n",
    "course_offerings.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412ae4a",
   "metadata": {},
   "source": [
    "This course offers table is great, but I also want to know who teaches each course because I want to check the reviews of the instructor before I take the course. Let's see if we can join this table with the instructors table that contains the name of the instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bfb8d07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|id     |name              |\n",
      "+-------+------------------+\n",
      "|761703 |JOHN ARCHAMBAULT  |\n",
      "|3677061|STEPHANIE KANN    |\n",
      "|788586 |KATHY PREM        |\n",
      "|1600463|KRISTIN KLARKOWSKI|\n",
      "+-------+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instructors.show(4,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52717f90",
   "metadata": {},
   "source": [
    "Hmmm, so this table only contains 2 columns (id and name) and doesn't have the uuid or course uuid to join on. So we will need to see how we can accomplish the join we need. It looks like from the tables we have, we would need to take the following steps to get the variables we need. \n",
    "\n",
    " - **course_offerings (CO):** uuid, course_uuid, term_code, name\n",
    " - **instructors (I):** id, name\n",
    " - **sections (S):** uuid, course_offering_uuid,room_uuid, schedule_uuid\n",
    " - **teachings (T):** instructor_id, section_uuid\n",
    " \n",
    " I.id --> T.instructor_id\n",
    "                \\/\n",
    "          T.section_uuid --> S.uuid\n",
    "                              \\/\n",
    "                             S.course_offering_uuid --> CO.uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e98816c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|instructor_id|        section_uuid|\n",
      "+-------------+--------------------+\n",
      "|       761703|45adf63c-48c9-365...|\n",
      "|       761703|c6280e23-5e43-385...|\n",
      "|       761703|9395dc21-15d1-3fa...|\n",
      "+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teachings.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6992da6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instructor_id</th>\n",
       "      <th>name</th>\n",
       "      <th>section_uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>761703</td>\n",
       "      <td>JOHN ARCHAMBAULT</td>\n",
       "      <td>45adf63c-48c9-3659-8561-07556d2d4ddf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>761703</td>\n",
       "      <td>JOHN ARCHAMBAULT</td>\n",
       "      <td>c6280e23-5e43-3859-893e-540d94993529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>761703</td>\n",
       "      <td>JOHN ARCHAMBAULT</td>\n",
       "      <td>9395dc21-15d1-3fab-8d1f-6f3fe6114c48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3677061</td>\n",
       "      <td>STEPHANIE KANN</td>\n",
       "      <td>b99e440b-39db-350a-81eb-b6eb1bd8b0bc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instructor_id              name                          section_uuid\n",
       "0         761703  JOHN ARCHAMBAULT  45adf63c-48c9-3659-8561-07556d2d4ddf\n",
       "1         761703  JOHN ARCHAMBAULT  c6280e23-5e43-3859-893e-540d94993529\n",
       "2         761703  JOHN ARCHAMBAULT  9395dc21-15d1-3fab-8d1f-6f3fe6114c48\n",
       "3        3677061    STEPHANIE KANN  b99e440b-39db-350a-81eb-b6eb1bd8b0bc"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try to see all course offerings and who teaches it\n",
    "# Notice here that the variable we want to join on is different in the two datasets. \n",
    "# PySpark makes it easy to account for that\n",
    "step1 = teachings.join(instructors, teachings.instructor_id == instructors.id, how='left').select(['instructor_id','name','section_uuid'])\n",
    "step1.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1b16dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>course_offering_uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THOMAS JAHNS</td>\n",
       "      <td>f850ab24-740c-311a-a669-804a3fea7b0b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JEAN-FRANCOIS HOUDE</td>\n",
       "      <td>7e213b2b-c58b-3014-b3d1-01c0f7ed46ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHRISTOPHER R TABER</td>\n",
       "      <td>3beb7bd7-4877-3c63-8afc-62f8b74e72fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MARISA S OTEGUI</td>\n",
       "      <td>db253216-2e66-3267-86b2-7b9f5fe07223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name                  course_offering_uuid\n",
       "0         THOMAS JAHNS  f850ab24-740c-311a-a669-804a3fea7b0b\n",
       "1  JEAN-FRANCOIS HOUDE  7e213b2b-c58b-3014-b3d1-01c0f7ed46ef\n",
       "2  CHRISTOPHER R TABER  3beb7bd7-4877-3c63-8afc-62f8b74e72fc\n",
       "3      MARISA S OTEGUI  db253216-2e66-3267-86b2-7b9f5fe07223"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2 = step1.join(sections, step1.section_uuid == sections.uuid, how='left').select(['name','course_offering_uuid'])\n",
    "step2.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d0a7417a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instructor</th>\n",
       "      <th>name</th>\n",
       "      <th>course_offering_uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THOMAS JAHNS</td>\n",
       "      <td>Master's Research or Thesis</td>\n",
       "      <td>f850ab24-740c-311a-a669-804a3fea7b0b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JEAN-FRANCOIS HOUDE</td>\n",
       "      <td>Wrkshp-Industrl Organizatn</td>\n",
       "      <td>7e213b2b-c58b-3014-b3d1-01c0f7ed46ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHRISTOPHER R TABER</td>\n",
       "      <td>Workshop - Public Economics</td>\n",
       "      <td>3beb7bd7-4877-3c63-8afc-62f8b74e72fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MARISA S OTEGUI</td>\n",
       "      <td>Plant Cell Biology</td>\n",
       "      <td>db253216-2e66-3267-86b2-7b9f5fe07223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            instructor                         name  \\\n",
       "0         THOMAS JAHNS  Master's Research or Thesis   \n",
       "1  JEAN-FRANCOIS HOUDE   Wrkshp-Industrl Organizatn   \n",
       "2  CHRISTOPHER R TABER  Workshop - Public Economics   \n",
       "3      MARISA S OTEGUI           Plant Cell Biology   \n",
       "\n",
       "                   course_offering_uuid  \n",
       "0  f850ab24-740c-311a-a669-804a3fea7b0b  \n",
       "1  7e213b2b-c58b-3014-b3d1-01c0f7ed46ef  \n",
       "2  3beb7bd7-4877-3c63-8afc-62f8b74e72fc  \n",
       "3  db253216-2e66-3267-86b2-7b9f5fe07223  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step3 = step2.withColumnRenamed('name', 'instructor').join(course_offerings, step2.course_offering_uuid == course_offerings.uuid, how='inner').select(['instructor','name','course_offering_uuid'])\n",
    "step3.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93e025",
   "metadata": {},
   "source": [
    "And that's it! Sometimes it's helpful to think through joins step by step like this. I hope that helped get the concept down. \n",
    "\n",
    "## One final really cool way to join datasets: The Levenshtien distance!\n",
    "\n",
    "Which basically counts the number of edits you would need to make to make too strings equal to eachother. I'll let you figure the joining part in the HW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bef1741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct this company name: Aple\n",
      "+-----+\n",
      "|Apple|\n",
      "+-----+\n",
      "|    1|\n",
      "+-----+\n",
      "\n",
      "+---------+\n",
      "|Microsoft|\n",
      "+---------+\n",
      "|        9|\n",
      "+---------+\n",
      "\n",
      "+---+\n",
      "|IBM|\n",
      "+---+\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the levenshtein distance beween two strings\n",
    "# pyspark.sql.functions.levenshtein(left, right)  \n",
    "\n",
    "from pyspark.sql.functions import levenshtein\n",
    "\n",
    "df0 = spark.createDataFrame([('Aple', 'Apple','Microsoft','IBM')], ['Input', 'Option1','Option2','Option3'])\n",
    "print(\"Correct this company name: Aple\")\n",
    "df0.select(levenshtein('Input', 'Option1').alias('Apple')).show()\n",
    "df0.select(levenshtein('Input', 'Option2').alias('Microsoft')).show()\n",
    "df0.select(levenshtein('Input', 'Option3').alias('IBM')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joins, all options \n",
    "\n",
    "inner_join = TableA.join(TableB, [\"name\",\"id\"],\"inner\")\n",
    "print(\"Inner Join Example\")\n",
    "print(inner_join.show())\n",
    "\n",
    "left_join = TableA.join(TableB, [\"name\",\"id\"], how='left') # Could also use 'left_outer'\n",
    "print(\"Left Join Example\")\n",
    "print(left_join.show())\n",
    "\n",
    "conditional_join = TableA.join(TableB, [\"name\",\"id\"], how='left').filter(TableB.name.isNull())\n",
    "print(\"Conditional Left Join\")\n",
    "print(conditional_join.show())\n",
    "\n",
    "right_join = TableA.join(TableB,  [\"name\",\"id\"],how='right') # Could also use 'right_outer'\n",
    "print(\"Right Join\")\n",
    "print(right_join.show())\n",
    "\n",
    "full_outer_join = TableA.join(TableB, [\"name\",\"id\"],how='full') # Could also use 'full_outer'\n",
    "print(\"Full Outer Join\")\n",
    "print(full_outer_join.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3baa26",
   "metadata": {},
   "source": [
    "# <span class=\"burk\">Handling missing data</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d2e98e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Some csv data\n",
    "zomato = spark.read.csv('../Datasets/zomato.csv',inferSchema=True,header=True)\n",
    "df = zomato.withColumn(\"approx_cost(for two people)\", zomato[\"approx_cost(for two people)\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"votes\", zomato[\"votes\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7dc1f63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                name|cuisines|\n",
      "+--------------------+--------+\n",
      "|               Jalsa|    null|\n",
      "|       Grand Village|    null|\n",
      "|       Casual Dining|    null|\n",
      "|     Timepass Dinner|    null|\n",
      "|       Casual Dining|    null|\n",
      "|Rosewood Internat...|    null|\n",
      "|       Casual Dining|    null|\n",
      "|              Onesta|    null|\n",
      "|      Penthouse Cafe|    null|\n",
      "|           Smacznego|    null|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.cuisines.isNull()).select(['name','cuisines']).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7414a6b4",
   "metadata": {},
   "source": [
    "**Missing Data Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6fec79b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>address</th>\n",
       "      <th>name</th>\n",
       "      <th>online_order</th>\n",
       "      <th>book_table</th>\n",
       "      <th>rate</th>\n",
       "      <th>votes</th>\n",
       "      <th>phone</th>\n",
       "      <th>location</th>\n",
       "      <th>rest_type</th>\n",
       "      <th>dish_liked</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>approx_cost(for two people)</th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>menu_item</th>\n",
       "      <th>listed_in(type)</th>\n",
       "      <th>listed_in(city)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>8111</td>\n",
       "      <td>2</td>\n",
       "      <td>7775</td>\n",
       "      <td>20018</td>\n",
       "      <td>1227</td>\n",
       "      <td>20054</td>\n",
       "      <td>20165</td>\n",
       "      <td>46841</td>\n",
       "      <td>27305</td>\n",
       "      <td>43611</td>\n",
       "      <td>28185</td>\n",
       "      <td>28611</td>\n",
       "      <td>28983</td>\n",
       "      <td>29344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>27.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>65.3</td>\n",
       "      <td>38.1</td>\n",
       "      <td>60.8</td>\n",
       "      <td>39.3</td>\n",
       "      <td>39.9</td>\n",
       "      <td>40.4</td>\n",
       "      <td>40.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   url address name online_order book_table  rate  votes phone location  \\\n",
       "0    0       0   85         8111          2  7775  20018  1227    20054   \n",
       "1  0.0     0.0  0.1         11.3        0.0  10.8   27.9   1.7     28.0   \n",
       "\n",
       "  rest_type dish_liked cuisines approx_cost(for two people) reviews_list  \\\n",
       "0     20165      46841    27305                       43611        28185   \n",
       "1      28.1       65.3     38.1                        60.8         39.3   \n",
       "\n",
       "  menu_item listed_in(type) listed_in(city)  \n",
       "0     28611           28983           29344  \n",
       "1      39.9            40.4            40.9  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "nulls = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "percent = df.select([format_number(((count(when(isnan(c) | col(c).isNull(), c))/df.count())*100),1).alias(c) for c in df.columns])\n",
    "\n",
    "result = nulls.union(percent)\n",
    "\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d43dbb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------+\n",
      "|Column_With_Null_Value|Null_Values_Count|\n",
      "+----------------------+-----------------+\n",
      "|                  name|               85|\n",
      "|          online_order|             8111|\n",
      "|            book_table|                2|\n",
      "|                  rate|             7775|\n",
      "|                 votes|            20018|\n",
      "|                 phone|             1227|\n",
      "|              location|            20054|\n",
      "|             rest_type|            20165|\n",
      "|            dish_liked|            46841|\n",
      "|              cuisines|            27305|\n",
      "|  approx_cost(for t...|            43611|\n",
      "|          reviews_list|            28185|\n",
      "|             menu_item|            28611|\n",
      "|       listed_in(type)|            28983|\n",
      "|       listed_in(city)|            29344|\n",
      "+----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "def null_value_count(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)\n",
    "\n",
    "null_columns_count_list = null_value_count(df)\n",
    "spark.createDataFrame(null_columns_count_list, ['Column_With_Null_Value', 'Null_Values_Count']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9a81b",
   "metadata": {},
   "source": [
    "**Drop all missing data**\n",
    "\n",
    "PySpark has a really handy .na function for working with missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "48a82883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>address</th>\n",
       "      <th>name</th>\n",
       "      <th>online_order</th>\n",
       "      <th>book_table</th>\n",
       "      <th>rate</th>\n",
       "      <th>votes</th>\n",
       "      <th>phone</th>\n",
       "      <th>location</th>\n",
       "      <th>rest_type</th>\n",
       "      <th>dish_liked</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>approx_cost(for two people)</th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>menu_item</th>\n",
       "      <th>listed_in(type)</th>\n",
       "      <th>listed_in(city)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.zomato.com/bangalore/spice-elephan...</td>\n",
       "      <td>2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...</td>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>4.1/5</td>\n",
       "      <td>787</td>\n",
       "      <td>080 41714161</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Casual Dining</td>\n",
       "      <td>Momos, Lunch Buffet, Chocolate Nirvana, Thai G...</td>\n",
       "      <td>Chinese, North Indian, Thai</td>\n",
       "      <td>800</td>\n",
       "      <td>\"[('Rated 4.0', 'RATED\\n  Had been here for di...</td>\n",
       "      <td>rice was well cooked and overall was great\\n\\n...</td>\n",
       "      <td>('Rated 5.0'</td>\n",
       "      <td>'RATED\\n  This place just cool ? with good am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.zomato.com/SanchurroBangalore?cont...</td>\n",
       "      <td>1112, Next to KIMS Medical College, 17th Cross...</td>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>3.8/5</td>\n",
       "      <td>918</td>\n",
       "      <td>+91 9663487993</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Cafe, Casual Dining</td>\n",
       "      <td>Churros, Cannelloni, Minestrone Soup, Hot Choc...</td>\n",
       "      <td>Cafe, Mexican, Italian</td>\n",
       "      <td>800</td>\n",
       "      <td>\"[('Rated 3.0', \"\"RATED\\n  Ambience is not tha...</td>\n",
       "      <td>('Rated 3.0'</td>\n",
       "      <td>\"\"RATED\\n \\nWent there for a quick bite with ...</td>\n",
       "      <td>pasta churros and lasagne.\\n\\nNachos were pat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.zomato.com/bangalore/spice-elephan...   \n",
       "1  https://www.zomato.com/SanchurroBangalore?cont...   \n",
       "\n",
       "                                             address             name  \\\n",
       "0  2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...   Spice Elephant   \n",
       "1  1112, Next to KIMS Medical College, 17th Cross...  San Churro Cafe   \n",
       "\n",
       "  online_order book_table   rate  votes           phone      location  \\\n",
       "0          Yes         No  4.1/5    787    080 41714161  Banashankari   \n",
       "1          Yes         No  3.8/5    918  +91 9663487993  Banashankari   \n",
       "\n",
       "             rest_type                                         dish_liked  \\\n",
       "0        Casual Dining  Momos, Lunch Buffet, Chocolate Nirvana, Thai G...   \n",
       "1  Cafe, Casual Dining  Churros, Cannelloni, Minestrone Soup, Hot Choc...   \n",
       "\n",
       "                      cuisines  approx_cost(for two people)  \\\n",
       "0  Chinese, North Indian, Thai                          800   \n",
       "1       Cafe, Mexican, Italian                          800   \n",
       "\n",
       "                                        reviews_list  \\\n",
       "0  \"[('Rated 4.0', 'RATED\\n  Had been here for di...   \n",
       "1  \"[('Rated 3.0', \"\"RATED\\n  Ambience is not tha...   \n",
       "\n",
       "                                           menu_item  \\\n",
       "0  rice was well cooked and overall was great\\n\\n...   \n",
       "1                                       ('Rated 3.0'   \n",
       "\n",
       "                                     listed_in(type)  \\\n",
       "0                                       ('Rated 5.0'   \n",
       "1   \"\"RATED\\n \\nWent there for a quick bite with ...   \n",
       "\n",
       "                                     listed_in(city)  \n",
       "0   'RATED\\n  This place just cool ? with good am...  \n",
       "1   pasta churros and lasagne.\\n\\nNachos were pat...  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop().limit(2).toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "23077e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 391:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Dropped: 63124\n",
      "Percentage of Rows Dropped 0.8800223058692318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 391:=================================================>       (7 + 1) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Of course you will want to know how many rows that affected before you actually execute it..\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop().count()\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f29fe519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Dropped: 1694\n",
      "Percentage of Rows Dropped 0.023616339049212325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 395:>                                                        (0 + 8) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Drop rows that have at least 8 NON-null values\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(thresh=8).count()\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6a51712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Dropped: 7775\n",
      "Percentage of Rows Dropped 0.10839258329848041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 399:==========================================>              (6 + 2) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Only drop the rows whose values in the sales column are null\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(subset=[\"rate\"]).count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "41355f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Dropped: 7775\n",
      "Percentage of Rows Dropped 0.10839258329848041\n"
     ]
    }
   ],
   "source": [
    "# Another way to do the above\n",
    "og_len = df.count()\n",
    "drop_len = df.filter(zomato.rate.isNotNull()).count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d3f4c815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Dropped: 0\n",
      "Percentage of Rows Dropped 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 407:>                                                        (0 + 8) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Drop a row only if ALL its values are null.\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(how='all').count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8b172",
   "metadata": {},
   "source": [
    "### Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "197b259f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>address</th>\n",
       "      <th>name</th>\n",
       "      <th>online_order</th>\n",
       "      <th>book_table</th>\n",
       "      <th>rate</th>\n",
       "      <th>votes</th>\n",
       "      <th>phone</th>\n",
       "      <th>location</th>\n",
       "      <th>rest_type</th>\n",
       "      <th>dish_liked</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>approx_cost(for two people)</th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>menu_item</th>\n",
       "      <th>listed_in(type)</th>\n",
       "      <th>listed_in(city)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.zomato.com/bangalore/jalsa-banasha...</td>\n",
       "      <td>942, 21st Main Road, 2nd Stage, Banashankari, ...</td>\n",
       "      <td>Jalsa</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4.1/5</td>\n",
       "      <td>775.0</td>\n",
       "      <td>080 42297555</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>MISSING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+91 9743772233\"</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Casual Dining</td>\n",
       "      <td>Pasta, Lunch Buffet, Masala Papad, Paneer Laja...</td>\n",
       "      <td>North Indian, Mughlai, Chinese</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>'RATED\\n  You canÃ\\x83Ã\\x83Ã\\x82Ã\\x82Ã\\x...</td>\n",
       "      <td>('Rated 5.0'</td>\n",
       "      <td>'RATED\\n  Overdelighted by the service and fo...</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>'RATED\\n  The place is nice and comfortable. ...</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>'RATED\\n  The place is nice and comfortable. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.zomato.com/bangalore/jalsa-banasha...   \n",
       "1                                    +91 9743772233\"   \n",
       "\n",
       "                                             address           name  \\\n",
       "0  942, 21st Main Road, 2nd Stage, Banashankari, ...          Jalsa   \n",
       "1                                       Banashankari  Casual Dining   \n",
       "\n",
       "                                        online_order  \\\n",
       "0                                                Yes   \n",
       "1  Pasta, Lunch Buffet, Masala Papad, Paneer Laja...   \n",
       "\n",
       "                       book_table   rate  votes          phone  \\\n",
       "0                             Yes  4.1/5  775.0   080 42297555   \n",
       "1  North Indian, Mughlai, Chinese    800    NaN   ('Rated 4.0'   \n",
       "\n",
       "                                            location      rest_type  \\\n",
       "0                                            MISSING        MISSING   \n",
       "1   'RATED\\n  You canÃ\\x83Ã\\x83Ã\\x82Ã\\x82Ã\\x...   ('Rated 5.0'   \n",
       "\n",
       "                                          dish_liked       cuisines  \\\n",
       "0                                            MISSING        MISSING   \n",
       "1   'RATED\\n  Overdelighted by the service and fo...   ('Rated 4.0'   \n",
       "\n",
       "   approx_cost(for two people)   reviews_list  \\\n",
       "0                          NaN        MISSING   \n",
       "1                          NaN   ('Rated 4.0'   \n",
       "\n",
       "                                           menu_item listed_in(type)  \\\n",
       "0                                            MISSING         MISSING   \n",
       "1   'RATED\\n  The place is nice and comfortable. ...    ('Rated 4.0'   \n",
       "\n",
       "                                     listed_in(city)  \n",
       "0                                            MISSING  \n",
       "1   'RATED\\n  The place is nice and comfortable. ...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill all nulls values with one common value (character value)\n",
    "df.na.fill('MISSING').limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8a51edf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>address</th>\n",
       "      <th>name</th>\n",
       "      <th>online_order</th>\n",
       "      <th>book_table</th>\n",
       "      <th>rate</th>\n",
       "      <th>votes</th>\n",
       "      <th>phone</th>\n",
       "      <th>location</th>\n",
       "      <th>rest_type</th>\n",
       "      <th>dish_liked</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>approx_cost(for two people)</th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>menu_item</th>\n",
       "      <th>listed_in(type)</th>\n",
       "      <th>listed_in(city)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.zomato.com/bangalore/jalsa-banasha...</td>\n",
       "      <td>942, 21st Main Road, 2nd Stage, Banashankari, ...</td>\n",
       "      <td>Jalsa</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4.1/5</td>\n",
       "      <td>775</td>\n",
       "      <td>080 42297555</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>999</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+91 9743772233\"</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Casual Dining</td>\n",
       "      <td>Pasta, Lunch Buffet, Masala Papad, Paneer Laja...</td>\n",
       "      <td>North Indian, Mughlai, Chinese</td>\n",
       "      <td>800</td>\n",
       "      <td>999</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>'RATED\\n  You canÃ\\x83Ã\\x83Ã\\x82Ã\\x82Ã\\x...</td>\n",
       "      <td>('Rated 5.0'</td>\n",
       "      <td>'RATED\\n  Overdelighted by the service and fo...</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>999</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>'RATED\\n  The place is nice and comfortable. ...</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>'RATED\\n  The place is nice and comfortable. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.zomato.com/bangalore/jalsa-banasha...   \n",
       "1                                    +91 9743772233\"   \n",
       "\n",
       "                                             address           name  \\\n",
       "0  942, 21st Main Road, 2nd Stage, Banashankari, ...          Jalsa   \n",
       "1                                       Banashankari  Casual Dining   \n",
       "\n",
       "                                        online_order  \\\n",
       "0                                                Yes   \n",
       "1  Pasta, Lunch Buffet, Masala Papad, Paneer Laja...   \n",
       "\n",
       "                       book_table   rate  votes          phone  \\\n",
       "0                             Yes  4.1/5    775   080 42297555   \n",
       "1  North Indian, Mughlai, Chinese    800    999   ('Rated 4.0'   \n",
       "\n",
       "                                            location      rest_type  \\\n",
       "0                                               None           None   \n",
       "1   'RATED\\n  You canÃ\\x83Ã\\x83Ã\\x82Ã\\x82Ã\\x...   ('Rated 5.0'   \n",
       "\n",
       "                                          dish_liked       cuisines  \\\n",
       "0                                               None           None   \n",
       "1   'RATED\\n  Overdelighted by the service and fo...   ('Rated 4.0'   \n",
       "\n",
       "   approx_cost(for two people)   reviews_list  \\\n",
       "0                          999           None   \n",
       "1                          999   ('Rated 4.0'   \n",
       "\n",
       "                                           menu_item listed_in(type)  \\\n",
       "0                                               None            None   \n",
       "1   'RATED\\n  The place is nice and comfortable. ...    ('Rated 4.0'   \n",
       "\n",
       "                                     listed_in(city)  \n",
       "0                                               None  \n",
       "1   'RATED\\n  The place is nice and comfortable. ...  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill all nulls values with one common value (numeric value)\n",
    "df.na.fill(999).limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6ad67",
   "metadata": {},
   "source": [
    "Usually you should specify what columns you want to fill with the subset parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bc3a93b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>address</th>\n",
       "      <th>name</th>\n",
       "      <th>online_order</th>\n",
       "      <th>book_table</th>\n",
       "      <th>rate</th>\n",
       "      <th>votes</th>\n",
       "      <th>phone</th>\n",
       "      <th>location</th>\n",
       "      <th>rest_type</th>\n",
       "      <th>dish_liked</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>approx_cost(for two people)</th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>menu_item</th>\n",
       "      <th>listed_in(type)</th>\n",
       "      <th>listed_in(city)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+91 9986692090\"</td>\n",
       "      <td>BTM</td>\n",
       "      <td>No Name</td>\n",
       "      <td>Momos, Oreo Shake</td>\n",
       "      <td>Mughlai, North Indian, Chinese, Momos</td>\n",
       "      <td>600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>('Rated 3.0'</td>\n",
       "      <td>'RATED\\n  Simple food with great north indian...</td>\n",
       "      <td>['Fry Chicken Kabab [5 Pieces]', 'Fry Chicken ...</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>Bannerghatta Road</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               url address     name       online_order  \\\n",
       "0  +91 9986692090\"     BTM  No Name  Momos, Oreo Shake   \n",
       "\n",
       "                              book_table rate  votes          phone  \\\n",
       "0  Mughlai, North Indian, Chinese, Momos  600    NaN   ('Rated 3.0'   \n",
       "\n",
       "                                            location  \\\n",
       "0   'RATED\\n  Simple food with great north indian...   \n",
       "\n",
       "                                           rest_type dish_liked  \\\n",
       "0  ['Fry Chicken Kabab [5 Pieces]', 'Fry Chicken ...   Delivery   \n",
       "\n",
       "            cuisines  approx_cost(for two people) reviews_list menu_item  \\\n",
       "0  Bannerghatta Road                          NaN         None      None   \n",
       "\n",
       "  listed_in(type) listed_in(city)  \n",
       "0            None            None  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.name.isNull()).na.fill('No Name',subset=['name']).limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f7f79",
   "metadata": {},
   "source": [
    "A very common practice is to fill values with the mean value for the column. Here is a fun function to that in an automatted way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0aa15a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>address</th>\n",
       "      <th>name</th>\n",
       "      <th>online_order</th>\n",
       "      <th>book_table</th>\n",
       "      <th>rate</th>\n",
       "      <th>votes</th>\n",
       "      <th>phone</th>\n",
       "      <th>location</th>\n",
       "      <th>rest_type</th>\n",
       "      <th>dish_liked</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>approx_cost(for two people)</th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>menu_item</th>\n",
       "      <th>listed_in(type)</th>\n",
       "      <th>listed_in(city)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.zomato.com/bangalore/jalsa-banasha...</td>\n",
       "      <td>942, 21st Main Road, 2nd Stage, Banashankari, ...</td>\n",
       "      <td>Jalsa</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4.1/5</td>\n",
       "      <td>775</td>\n",
       "      <td>080 42297555</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>387</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+91 9743772233\"</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Casual Dining</td>\n",
       "      <td>Pasta, Lunch Buffet, Masala Papad, Paneer Laja...</td>\n",
       "      <td>North Indian, Mughlai, Chinese</td>\n",
       "      <td>800</td>\n",
       "      <td>283</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>'RATED\\n  You canÃ\\x83Ã\\x83Ã\\x82Ã\\x82Ã\\x...</td>\n",
       "      <td>('Rated 5.0'</td>\n",
       "      <td>'RATED\\n  Overdelighted by the service and fo...</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>387</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>'RATED\\n  The place is nice and comfortable. ...</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>'RATED\\n  The place is nice and comfortable. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.zomato.com/bangalore/jalsa-banasha...   \n",
       "1                                    +91 9743772233\"   \n",
       "\n",
       "                                             address           name  \\\n",
       "0  942, 21st Main Road, 2nd Stage, Banashankari, ...          Jalsa   \n",
       "1                                       Banashankari  Casual Dining   \n",
       "\n",
       "                                        online_order  \\\n",
       "0                                                Yes   \n",
       "1  Pasta, Lunch Buffet, Masala Papad, Paneer Laja...   \n",
       "\n",
       "                       book_table   rate  votes          phone  \\\n",
       "0                             Yes  4.1/5    775   080 42297555   \n",
       "1  North Indian, Mughlai, Chinese    800    283   ('Rated 4.0'   \n",
       "\n",
       "                                            location      rest_type  \\\n",
       "0                                               None           None   \n",
       "1   'RATED\\n  You canÃ\\x83Ã\\x83Ã\\x82Ã\\x82Ã\\x...   ('Rated 5.0'   \n",
       "\n",
       "                                          dish_liked       cuisines  \\\n",
       "0                                               None           None   \n",
       "1   'RATED\\n  Overdelighted by the service and fo...   ('Rated 4.0'   \n",
       "\n",
       "   approx_cost(for two people)   reviews_list  \\\n",
       "0                          387           None   \n",
       "1                          387   ('Rated 4.0'   \n",
       "\n",
       "                                           menu_item listed_in(type)  \\\n",
       "0                                               None            None   \n",
       "1   'RATED\\n  The place is nice and comfortable. ...    ('Rated 4.0'   \n",
       "\n",
       "                                     listed_in(city)  \n",
       "0                                               None  \n",
       "1   'RATED\\n  The place is nice and comfortable. ...  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fill_with_mean(df, include=set()): \n",
    "    stats = df.agg(*(\n",
    "        avg(c).alias(c) for c in df.columns if c in include\n",
    "    ))\n",
    "#     stats = stats.select(*(col(c).cast(\"int\").alias(c) for c in stats.columns)) #IntegerType()\n",
    "    return df.na.fill(stats.first().asDict())\n",
    "\n",
    "updated_df = fill_with_mean(df, [\"approx_cost(for two people)\",\"votes\"])\n",
    "updated_df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4bf434",
   "metadata": {},
   "source": [
    "# <span class=\"burk\">Manipulating Data in DataFrames</span>\n",
    "\n",
    "Change data types\n",
    "\n",
    "### Available types:\n",
    "    - DataType\n",
    "    - NullType\n",
    "    - StringType\n",
    "    - BinaryType\n",
    "    - BooleanType\n",
    "    - DateType\n",
    "    - TimestampType\n",
    "    - DecimalType\n",
    "    - DoubleType\n",
    "    - FloatType\n",
    "    - ByteType\n",
    "    - IntegerType\n",
    "    - LongType\n",
    "    - ShortType\n",
    "    - ArrayType\n",
    "    - MapType\n",
    "    - StructField\n",
    "    - StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9b1ca",
   "metadata": {},
   "source": [
    "**Spark Immutabale**\n",
    "\n",
    "Before we get started, let's first take a moment to discuss the concept of Sparks Immutability. Spark DataFrames are immutable. What does that mean? Let's take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "65faae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|   Abraham|  Lincoln|\n",
      "+----------+---------+\n",
      "\n",
      "None\n",
      "1127\n"
     ]
    }
   ],
   "source": [
    "names = spark.createDataFrame([('Abraham','Lincoln')], ['first_name', 'last_name'])\n",
    "print(names.show())\n",
    "print(names.rdd.id())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207c951",
   "metadata": {},
   "source": [
    "Note the dataframe id\n",
    "\n",
    "Now add a column to the dataframe and keep calling it the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a35a1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a col\n",
    "from pyspark.sql.functions import *\n",
    "names = names.select(names.first_name,names.last_name,concat_ws(' ', names.first_name, names.last_name).alias('full_name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55f780",
   "metadata": {},
   "source": [
    "And see how the id of the dataframe changes but the name of the dataframe is still the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "336f860b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+\n",
      "|first_name|last_name|      full_name|\n",
      "+----------+---------+---------------+\n",
      "|   Abraham|  Lincoln|Abraham Lincoln|\n",
      "+----------+---------+---------------+\n",
      "\n",
      "None\n",
      "1133\n"
     ]
    }
   ],
   "source": [
    "print(names.show())\n",
    "print(names.rdd.id())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf0758",
   "metadata": {},
   "source": [
    "**Read in our Data Science Jobs DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc149cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:=======>                                                   (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path='../Datasets/'\n",
    "videos = spark.read.csv(path+'youtubevideos.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e38c44c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: date (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: string (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T17:13:01.000Z</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>57527</td>\n",
       "      <td>2966</td>\n",
       "      <td>15954</td>\n",
       "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date                               title  \\\n",
       "0  2kyS6SvSYSE    2011-01-17  WE WANT TO TALK ABOUT OUR MARRIAGE   \n",
       "\n",
       "  channel_title category_id              publish_time             tags  \\\n",
       "0  CaseyNeistat          22  2017-11-13T17:13:01.000Z  SHANtell martin   \n",
       "\n",
       "    views  likes  dislikes comment_count  \\\n",
       "0  748374  57527      2966         15954   \n",
       "\n",
       "                                   thumbnail_link comments_disabled  \\\n",
       "0  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg             False   \n",
       "\n",
       "  ratings_disabled video_error_or_removed  \\\n",
       "0            False                  False   \n",
       "\n",
       "                                         description  \n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice all vars are stings above....\n",
    "# let's change that\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * # IntegerType\n",
    "\n",
    "df = videos.withColumn(\"views\", videos[\"views\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"likes\", videos[\"likes\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"dislikes\", videos[\"dislikes\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"trending_date\", to_date(videos.trending_date, 'dd.mm.yy')) \\\n",
    "#         .withColumn(\"publish_time\", to_timestamp(videos.publish_time, 'yyyy-MM-dd HH:mm:ss:ms'))\n",
    "print(df.printSchema())\n",
    "df.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cd5cd",
   "metadata": {},
   "source": [
    "**Regex**\n",
    "\n",
    "Regex is used to replace or extract all substrings of the specified string value that match regexp with rep.\n",
    "regexp_replace(str, pattern, replacement)\n",
    "for more info on regex calls visit: https://docs.oracle.com/cd/B19306_01/server.102/b14200/ap_posix001.htm#BABJDBHB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ed4f5",
   "metadata": {},
   "source": [
    "**Renaming Columns**\n",
    "\n",
    "If you simply needed to rename a column you use also use this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ea8cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title_new</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T17:13:01.000Z</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>57527</td>\n",
       "      <td>2966</td>\n",
       "      <td>15954</td>\n",
       "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T07:30:00.000Z</td>\n",
       "      <td>\"last week tonight trump presidency\"|\"last wee...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>97185</td>\n",
       "      <td>6146</td>\n",
       "      <td>12703</td>\n",
       "      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>One year after the presidential election, John...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE    2011-01-17   \n",
       "1  1ZAPwfrtAFY    2011-01-17   \n",
       "\n",
       "                                               title channel_title_new  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE      CaseyNeistat   \n",
       "1  The Trump Presidency: Last Week Tonight with J...   LastWeekTonight   \n",
       "\n",
       "  category_id              publish_time  \\\n",
       "0          22  2017-11-13T17:13:01.000Z   \n",
       "1          24  2017-11-13T07:30:00.000Z   \n",
       "\n",
       "                                                tags    views  likes  \\\n",
       "0                                    SHANtell martin   748374  57527   \n",
       "1  \"last week tonight trump presidency\"|\"last wee...  2418783  97185   \n",
       "\n",
       "   dislikes comment_count                                  thumbnail_link  \\\n",
       "0      2966         15954  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n",
       "1      6146         12703  https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg   \n",
       "\n",
       "  comments_disabled ratings_disabled video_error_or_removed  \\\n",
       "0             False            False                  False   \n",
       "1             False            False                  False   \n",
       "\n",
       "                                         description  \n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  \n",
       "1  One year after the presidential election, John...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Rename\n",
    "renamed = df.withColumnRenamed('channel_title','channel_title_new')\n",
    "renamed.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0cb414",
   "metadata": {},
   "source": [
    "**Clean Data**\n",
    "\n",
    "Alright so we see that the publish_time variable could not be converted to a timestamp becuase it has those strange \"T\" and \"Z\" values between the date and the time. We essentially need to replace the \"T\" value with a space, and the Z value with nothing. There are a couple of ways we can do this, the first is regex which is short for regular expressions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "524912a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------+-------------------+\n",
      "|publish_time            |publish_time_2         |publish_time_3     |\n",
      "+------------------------+-----------------------+-------------------+\n",
      "|2017-11-13T17:13:01.000Z|2017-11-13 17:13:01.000|2017-11-13 17:13:01|\n",
      "|2017-11-13T07:30:00.000Z|2017-11-13 07:30:00.000|2017-11-13 07:30:00|\n",
      "|2017-11-12T19:05:24.000Z|2017-11-12 19:05:24.000|2017-11-12 19:05:24|\n",
      "|2017-11-13T11:00:04.000Z|2017-11-13 11:00:04.000|2017-11-13 11:00:04|\n",
      "|2017-11-12T18:01:41.000Z|2017-11-12 18:01:41.000|2017-11-12 18:01:41|\n",
      "+------------------------+-----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, regexp_extract\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "df = df.withColumn('publish_time_2',regexp_replace(df.publish_time, 'T', ' '))\n",
    "df = df.withColumn('publish_time_2',regexp_replace(df.publish_time_2, 'Z', ''))\n",
    "df = df.withColumn(\"publish_time_3\", to_timestamp(df.publish_time_2, 'yyyy-MM-dd HH:mm:ss.SSS'))\n",
    "#print(df.printSchema())\n",
    "df.select(\"publish_time\", \"publish_time_2\",\"publish_time_3\").show(5,False)\n",
    "# Notice the .000 on the end of publish_time_new as opposed to publish_time_new_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9e0ed",
   "metadata": {},
   "source": [
    "**Translate Function**\n",
    "\n",
    "You could also use the Translate function here to do this, where the first set of values is what you are looking for and the second set is what you want to replace those values with respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b0a29c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|                 A|          replaced|\n",
      "+------------------+------------------+\n",
      "|           $100,00|           X100Z00|\n",
      "|           #foobar|           Yfoobar|\n",
      "|foo, bar, #, and $|fooZ barZ YZ and X|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# You can also use the translate function for cases like this \n",
    "# where you wanted to replace ('$', '#', ',') with ('X', 'Y', 'Z')\n",
    "import pyspark.sql.functions as f\n",
    "foobar = spark.createDataFrame([(\"$100,00\",),(\"#foobar\",),(\"foo, bar, #, and $\",)], [\"A\"])\n",
    "foobar.select(\"A\", f.translate(f.col(\"A\"), \"$#,\", \"XYZ\").alias(\"replaced\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787a50e",
   "metadata": {},
   "source": [
    "**Trim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d0148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|          d1|          d2|\n",
      "+------------+------------+\n",
      "| 2015-04-08 | 2015-05-10 |\n",
      "+------------+------------+\n",
      "\n",
      "left trim\n",
      "+------------+-----------+\n",
      "|          d1|  ltrim(d1)|\n",
      "+------------+-----------+\n",
      "| 2015-04-08 |2015-04-08 |\n",
      "+------------+-----------+\n",
      "\n",
      "right trim\n",
      "+------------+-----------+\n",
      "|          d1|  rtrim(d1)|\n",
      "+------------+-----------+\n",
      "| 2015-04-08 | 2015-04-08|\n",
      "+------------+-----------+\n",
      "\n",
      "trim\n",
      "+------------+----------+\n",
      "|          d1|  trim(d1)|\n",
      "+------------+----------+\n",
      "| 2015-04-08 |2015-04-08|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trim\n",
    "# pyspark.sql.functions.trim(col) - Trim the spaces from both ends for the specified string column.\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "trim_ex = spark.createDataFrame([(' 2015-04-08 ',' 2015-05-10 ')], ['d1', 'd2']) # create a dataframe - notice the extra whitespaces in the date strings\n",
    "trim_ex.show()\n",
    "print(\"left trim\")\n",
    "trim_ex.select('d1', ltrim(trim_ex.d1)).show()\n",
    "print(\"right trim\")\n",
    "trim_ex.select('d1', rtrim(trim_ex.d1)).show()\n",
    "print(\"trim\")\n",
    "trim_ex.select('d1', trim(trim_ex.d1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e65df",
   "metadata": {},
   "source": [
    "**Case When**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb331824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Dataframe:\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  2|    2|\n",
      "|  3|    3|\n",
      "+---+-----+\n",
      "\n",
      "Option#1: withColumn() using when-otherwise\n",
      "+---+-----+----------+\n",
      "| id|value|value_desc|\n",
      "+---+-----+----------+\n",
      "|  1|    1|       one|\n",
      "|  2|    2|       two|\n",
      "|  3|    3|     other|\n",
      "+---+-----+----------+\n",
      "\n",
      "Option2: withColumn() using expr function\n",
      "+---+-----+----------+\n",
      "| id|value|value_desc|\n",
      "+---+-----+----------+\n",
      "|  1|    1|       one|\n",
      "|  2|    2|       two|\n",
      "|  3|    3|     other|\n",
      "+---+-----+----------+\n",
      "\n",
      "Option 3: selectExpr() using SQL equivalent CASE expression\n",
      "+---+-----+----------+\n",
      "| id|value|value_desc|\n",
      "+---+-----+----------+\n",
      "|  1|    1|       one|\n",
      "|  2|    2|       two|\n",
      "|  3|    3|     other|\n",
      "+---+-----+----------+\n",
      "\n",
      "Option 4: select() using expr function\n",
      "+---+-----+----------+\n",
      "| id|value|value_desc|\n",
      "+---+-----+----------+\n",
      "|  1|    1|       one|\n",
      "|  2|    2|       two|\n",
      "|  3|    3|     other|\n",
      "+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,1),(2,2),(3,3)],['id','value'])\n",
    "\n",
    "print(\"Sample Dataframe:\")\n",
    "df.show()\n",
    "\n",
    "print(\"Option#1: withColumn() using when-otherwise\")\n",
    "from pyspark.sql.functions import when\n",
    "df.withColumn(\"value_desc\",when(df.value == 1, 'one').when(df.value == 2, 'two').otherwise('other')).show()\n",
    "\n",
    "print(\"Option2: withColumn() using expr function\")\n",
    "from pyspark.sql.functions import expr \n",
    "df.withColumn(\"value_desc\",expr(\"CASE WHEN value == 1 THEN  'one' WHEN value == 2 THEN  'two' ELSE 'other' END AS value_desc\")).show()\n",
    "\n",
    "print(\"Option 3: selectExpr() using SQL equivalent CASE expression\")\n",
    "df.selectExpr(\"*\",\"CASE WHEN value == 1 THEN  'one' WHEN value == 2 THEN  'two' ELSE 'other' END AS value_desc\").show()\n",
    "\n",
    "print(\"Option 4: select() using expr function\")\n",
    "from pyspark.sql.functions import expr \n",
    "df.select(\"*\",expr(\"CASE WHEN value == 1 THEN  'one' WHEN value == 2 THEN  'two' ELSE 'other' END AS value_desc\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf7eff",
   "metadata": {},
   "source": [
    "**Lower**\n",
    "\n",
    "Another common data cleaning technique is lower casing all values in a string. Here's how we could do that.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85db4d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|title                                                         |\n",
      "+--------------------------------------------------------------+\n",
      "|we want to talk about our marriage                            |\n",
      "|the trump presidency: last week tonight with john oliver (hbo)|\n",
      "|racist superman | rudy mancuso, king bach & lele pons         |\n",
      "|nickelback lyrics: real or fake?                              |\n",
      "|i dare you: going bald!?                                      |\n",
      "+--------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('title',lower(df.title)) # or rtrim/ltrim\n",
    "df.select(\"title\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d88e9d",
   "metadata": {},
   "source": [
    "**Concatenate**\n",
    "\n",
    "If you want to combine two variables together (given a separator) you can use the concatenate method. Let's say we wanted to combined all the text description variables of the videos here for a robust NLP exercise of some sort and we needed to have all the text in one colum to do that like this.\n",
    "\n",
    "    concat_ws(sep, *cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49d74e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|text                                                           |\n",
      "+---------------------------------------------------------------+\n",
      "|we want to talk about our marriage CaseyNeistat SHANtell martin|\n",
      "+---------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "df.select(concat_ws(' ', df.title,df.channel_title,df.tags).alias('text')).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c2305",
   "metadata": {},
   "source": [
    "**Extracting data from Date and Timestamp variables**\n",
    "\n",
    "If you have the need to extract say the year or month from a date field, you can use PySpark's SQL function library like this. \n",
    "\n",
    "Note with this analysis we stumbled apon a date conversion descrepancy here. I'll leave fixing that for a hw problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92b61f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------------------+\n",
      "|trending_date|year(trending_date)|month(trending_date)|\n",
      "+-------------+-------------------+--------------------+\n",
      "|   2011-01-17|               2011|                   1|\n",
      "|   2011-01-17|               2011|                   1|\n",
      "|   2011-01-17|               2011|                   1|\n",
      "|   2011-01-17|               2011|                   1|\n",
      "|   2011-01-17|               2011|                   1|\n",
      "+-------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "# Other options: dayofmonth, dayofweek, dayofyear, weekofyear\n",
    "df.select(\"trending_date\",year(\"trending_date\"),month(\"trending_date\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7de38a",
   "metadata": {},
   "source": [
    "**Calculate the Difference between two dates**\n",
    "\n",
    "If you want to calculate the time difference between two dates, you could use PySparks datediff function which returns the number of days from start to end.\n",
    "\n",
    "    datediff(end, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bd68c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|difference is:|\n",
      "+--------------+\n",
      "|            32|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the difference between two dates:\n",
    "# pyspark.sql.functions.datediff(end, start)\n",
    "# Returns the number of days from start to end.\n",
    "\n",
    "date_df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
    "date_df.select(datediff(date_df.d2, date_df.d1).alias('difference ')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "df.select(\"trending_date\",\"publish_time_3\",(datediff(df.trending_date,df.publish_time_3)/365).alias('diff')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d3aa3",
   "metadata": {},
   "source": [
    "**Split a string around a pattern**\n",
    "\n",
    "If you ever need to split a string on a pattern (where the pattern is a regex), you could use PySparks split function. You could actually use this for tokenizing text which is an NLP function that we'll get into later.\n",
    "\n",
    "    df.select(split(str, pattern))\n",
    "    \n",
    "*Note that this will create an array*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4abae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|     s|    news|\n",
      "+------+--------+\n",
      "|ab12cd|[ab, cd]|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split a string around pattern (pattern is a regular expression).\n",
    "from pyspark.sql.functions import *\n",
    "# pyspark.sql.functions.split(str, pattern)[source]\n",
    "\n",
    "abc = spark.createDataFrame([('ab12cd',)], ['s',])\n",
    "abc.select(abc.s,split(abc.s, '[0-9]+').alias('news')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d018fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|title                             |\n",
      "+----------------------------------+\n",
      "|we want to talk about our marriage|\n",
      "+----------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------------------------------------+\n",
      "|new                                       |\n",
      "+------------------------------------------+\n",
      "|[we, want, to, talk, about, our, marriage]|\n",
      "+------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split a string around pattern (pattern is a regular expression).\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df.select(\"title\").show(1,False)\n",
    "df.select(split(df.title, ' ').alias('new')).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec1ee0",
   "metadata": {},
   "source": [
    "**Working with Arrays**\n",
    "\n",
    "    df.select(array_contains(df.variable, \"marriage\"))\n",
    "\n",
    "*note this is only available in pyspark 2.4+* \n",
    "    \n",
    "\n",
    " - .array(*cols)   -   Creates a new array column.\n",
    " - .array_contains(col, value)  - Collection function: returns null if the array is null, true if the array contains the given value, and false otherwise.\n",
    " - .array_distinct(col) - Collection function: removes duplicate values from the array. :param col: name of column or expression\n",
    " - .array_except(col1, col2) - Collection function: returns an array of the elements in col1 but not in col2, without duplicates.\n",
    " - .array_intersect(col1, col2) - Collection function: returns an array of the elements in the intersection of col1 and col2, without duplicates.\n",
    " - .array_join(col, delimiter, null_replacement=None) - Concatenates the elements of column using the delimiter. Null values are replaced with null_replacement if set, otherwise they are ignored.\n",
    " - .array_max(col) - Collection function: returns the maximum value of the array.\n",
    " - .array_min(col) - Collection function: returns the minimum value of the array.\n",
    " - .array_position(col, value) - Collection function: Locates the position of the first occurrence of the given value in the given array. Returns null if either of the arguments are null.\n",
    " - .array_remove(col, element)- Collection function: Remove all elements that equal to element from the given array.\n",
    " - .array_repeat(col, count) - Collection function: creates an array containing a column repeated count times.\n",
    " - .array_sort(col) - Collection function: sorts the input array in ascending order. The elements of the input array must be orderable. Null elements will be placed at the end of the returned array.\n",
    " - .array_union(col1, col2) - Collection function: returns an array of the elements in the union of col1 and col2, without duplicates.\n",
    " - .arrays_overlap(a1, a2) - Collection function: returns true if the arrays contain any common non-null element; if not, returns null if both the arrays are non-empty and any of them contains a null element; returns false otherwise.\n",
    " - .arrays_zip(*cols)[source] - Collection function: Returns a merged array of structs in which the N-th struct contains all N-th values of input arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2644399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array\n",
      "+--------------------+--------------------+\n",
      "|              Monday|             Tuesday|\n",
      "+--------------------+--------------------+\n",
      "|[coffee, milk, co...|[coffee, chocolat...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "Which customers purchased milk? array_contains\n",
      "+----------------------------+\n",
      "|array_contains(Monday, milk)|\n",
      "+----------------------------+\n",
      "|true                        |\n",
      "+----------------------------+\n",
      "\n",
      "List of unique products purchased on Monday: array_distinct\n",
      "+----------------------+\n",
      "|array_distinct(Monday)|\n",
      "+----------------------+\n",
      "|[coffee, milk]        |\n",
      "+----------------------+\n",
      "\n",
      "What did our customers order on Monday but not Tuesday? array_except\n",
      "+-----------------------------+\n",
      "|array_except(Monday, Tuesday)|\n",
      "+-----------------------------+\n",
      "|[milk]                       |\n",
      "+-----------------------------+\n",
      "\n",
      "What did our customers order on BOTH Monday and Tuesday?: array_intersect\n",
      "+--------------------------------+\n",
      "|array_intersect(Monday, Tuesday)|\n",
      "+--------------------------------+\n",
      "|[coffee]                        |\n",
      "+--------------------------------+\n",
      "\n",
      "All purchases on monday in a string: array_join\n",
      "+---------------------+\n",
      "|array_join(Monday, ,)|\n",
      "+---------------------+\n",
      "|coffee,milk,coffee   |\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "customer = spark.createDataFrame([('coffee','milk','coffee','coffee','chocolate','')], ['item1', 'item2','item3','item4','item5','item6'])\n",
    "purchases = customer.select(array('item1', 'item2','item3').alias(\"Monday\"),\\\n",
    "                            array('item4', 'item5','item6').alias(\"Tuesday\"))\n",
    "\n",
    "print(\"array\")\n",
    "purchases.show()\n",
    "\n",
    "print(\"Which customers purchased milk? array_contains\")\n",
    "purchases.select(array_contains(purchases.Monday, \"milk\")).show(1, False)\n",
    "\n",
    "print(\"List of unique products purchased on Monday: array_distinct\")\n",
    "purchases.select(array_distinct(purchases.Monday)).show(1, False)\n",
    "\n",
    "print(\"What did our customers order on Monday but not Tuesday? array_except\")\n",
    "purchases.select(array_except(purchases.Monday, purchases.Tuesday)).show(1, False)\n",
    "\n",
    "print(\"What did our customers order on BOTH Monday and Tuesday?: array_intersect\")\n",
    "purchases.select(array_intersect(purchases.Monday, purchases.Tuesday)).show(1, False)\n",
    "\n",
    "print(\"All purchases on monday in a string: array_join\")\n",
    "purchases.select(array_join(purchases.Monday, ',')).show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ff50189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-------------------------------------+\n",
      "|title                             |array_contains(title_array, marriage)|\n",
      "+----------------------------------+-------------------------------------+\n",
      "|we want to talk about our marriage|true                                 |\n",
      "+----------------------------------+-------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------------------------------------+\n",
      "|array_distinct(title_array)               |\n",
      "+------------------------------------------+\n",
      "|[we, want, to, talk, about, our, marriage]|\n",
      "+------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------------+\n",
      "|array_remove(title_array, we)         |\n",
      "+--------------------------------------+\n",
      "|[want, to, talk, about, our, marriage]|\n",
      "+--------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "array_df = df.select(\"title\",split(df.title, ' ').alias('title_array'))\n",
    "\n",
    "array_df.select(\"title\",array_contains(array_df.title_array, \"marriage\")).show(1, False)\n",
    "\n",
    "#get rid of repeat values\n",
    "array_df.select(array_distinct(array_df.title_array)).show(1, False)\n",
    "\n",
    "# Remove certian values\n",
    "array_df.select(array_remove(array_df.title_array, \"we\")).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84013052",
   "metadata": {},
   "source": [
    "## Creating Functions\n",
    "\n",
    "Functions as you know them in Python work a bit differently in Pyspark because it operates on a cluster. If you define a function the traditional Python way in PySpark, you will not recieve an error message but the call will not distribute on all nodes. So it will run slower. \n",
    "\n",
    "So to convert a Python function to what's called a user defined function (UDF) in PySpark. This is what you do.\n",
    "\n",
    "*Note: keep in mind that a function will not work on a column with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6c331c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|dislikes|likes_sq|\n",
      "+--------+--------+\n",
      "|    2966| 8797156|\n",
      "|    6146|37773316|\n",
      "|    5339|28504921|\n",
      "|     666|  443556|\n",
      "|    1989| 3956121|\n",
      "|     511|  261121|\n",
      "|    2445| 5978025|\n",
      "|     778|  605284|\n",
      "|     119|   14161|\n",
      "|    1363| 1857769|\n",
      "|      25|     625|\n",
      "|     303|   91809|\n",
      "|    1333| 1776889|\n",
      "|    1171| 1371241|\n",
      "|     246|   60516|\n",
      "|      52|    2704|\n",
      "|     638|  407044|\n",
      "|      53|    2809|\n",
      "|      36|    1296|\n",
      "|     191|   36481|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def square(x):\n",
    "    return int(x**2)\n",
    "square_udf = udf(lambda z: square(z), IntegerType())\n",
    "\n",
    "df.select('dislikes',square_udf('dislikes').alias('likes_sq')).where(col('dislikes').isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b8fae24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------------+\n",
      "|age|sentence            |array                     |\n",
      "+---+--------------------+--------------------------+\n",
      "|45 |I like to ride bikes|[I, like, to, ride, bikes]|\n",
      "+---+--------------------+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "values = [(45,'I like to ride bikes'), \\\n",
    "          (14,'I like chicken'), \\\n",
    "          (63,'I like bubbles'), \\\n",
    "          (75,'I like roller coasters'), \\\n",
    "          (24,'I like shuffle board'), \\\n",
    "          (45,'I like to swim')]\n",
    "sentences = spark.createDataFrame(values,['age', 'sentence'])\n",
    "df = sentences.withColumn(\"array\", split(col(\"sentence\"), \" \"))\n",
    "df.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1001b0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4cdb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c57651e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368a836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "401.438px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 731.4,
   "position": {
    "height": "753.4px",
    "left": "1578.2px",
    "right": "20px",
    "top": "115px",
    "width": "415px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
